{"cells":[{"cell_type":"markdown","metadata":{"id":"hSJCDeRPn89q"},"source":["# 0.&nbsp;들어가며"]},{"cell_type":"markdown","metadata":{"id":"RaQ73w479qdq"},"source":["## 0-1. 사전안내\n","- 텍스트 셀에 써있는 지침 반드시 잘 읽고 따르기\n","- 코드 셀의 주석 부분은 궁금하면 참고로 읽어보기"]},{"cell_type":"markdown","metadata":{"id":"sca6VgTzoGDO"},"source":["## 0-2. 데이터 준비\n","- roboflow에서 아래와 같은 형식으로 데이터를 다운받아 드라이브에 옮기기  \n","  - 훈련 데이터: Other - Tensorflow TFRecord\n","  - 테스트 데이터: XML - Pascar VOC"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"5oVo9SzJqjT6","colab":{"base_uri":"https://localhost:8080/","height":147},"executionInfo":{"status":"error","timestamp":1733115111057,"user_tz":-540,"elapsed":383,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}},"outputId":"d3ebe509-8f63-4595-8153-6055c9d9ff03"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'drive' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3b8a479202a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"LiK39Hce2EVV"},"source":["# 1.&nbsp;환경 설정 및 라이브러리 설치"]},{"cell_type":"markdown","metadata":{"id":"WigsNWfZ8LgF"},"source":["## 1-1. TensorFlow Object Detection API 설치"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13269,"status":"ok","timestamp":1733107130444,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"FSVgcMELyd15","outputId":"8e97f0ab-c130-4872-ea1a-44fa134d4da4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: Cython 3.0.11\n","Uninstalling Cython-3.0.11:\n","  Successfully uninstalled Cython-3.0.11\n","Cloning into 'models'...\n","remote: Enumerating objects: 4305, done.\u001b[K\n","remote: Counting objects: 100% (4305/4305), done.\u001b[K\n","remote: Compressing objects: 100% (3315/3315), done.\u001b[K\n","remote: Total 4305 (delta 1211), reused 2190 (delta 917), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (4305/4305), 53.16 MiB | 24.00 MiB/s, done.\n","Resolving deltas: 100% (1211/1211), done.\n"]}],"source":["# TensorFlow 모델 레포지토리를 GitHub에서 클론\n","# (TensorFlow Object Detection API 설치를 위해 필요)\n","!pip uninstall Cython -y  # \"No module named 'object_detection'\" 오류를 임시로 해결하기 위해 Cython을 제거\n","!git clone --depth 1 https://github.com/tensorflow/models  # TensorFlow 모델 소스 코드를 가져옴 (최신 커밋만 클론)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gxTHk6MzyiW_","executionInfo":{"status":"ok","timestamp":1733107130445,"user_tz":-540,"elapsed":14,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}}},"outputs":[],"source":["# 모델 설정 파일을 models/research 폴더로 복사 및 프로토콜 버퍼 파일 컴파일\n","\n","# Bash 셸 명령어 실행\n","%%bash\n","cd models/research/\n","\n","# Object Detection API에서 사용하는 .proto 파일을 컴파일하여 Python 코드로 변환\n","protoc object_detection/protos/*.proto --python_out=."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"p_2k5LHr2Lgb","executionInfo":{"status":"ok","timestamp":1733107130445,"user_tz":-540,"elapsed":11,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}}},"outputs":[],"source":["# setup.py 파일 수정: TensorFlow 모델 레포지토리를 TF v2.8.0에 맞춰 설치\n","import re  # 정규 표현식을 사용하기 위한 모듈\n","\n","# 기존 setup.py 파일의 내용을 읽어옴\n","with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n","    s = f.read()  # setup.py 파일 내용을 문자열로 읽기\n","\n","# 수정된 내용을 새로운 setup.py 파일에 작성\n","with open('/content/models/research/setup.py', 'w') as f:\n","    # \"tf-models-official>=2.5.1\" 문자열을 \"tf-models-official==2.8.0\"으로 변경\n","    # TensorFlow 버전 호환성을 위해 특정 버전으로 고정\n","    s = re.sub('tf-models-official>=2.5.1', 'tf-models-official==2.8.0', s)\n","    f.write(s)  # 수정된 내용을 새로운 setup.py 파일에 저장\n"]},{"cell_type":"markdown","metadata":{"id":"OZ8NFihJ2RGT"},"source":["- 아래 셀 실행 시 발생하는 **의존성 오류는 무시**\n","  \n","- 중간에 '**세션 다시 시작**' 팝업이 뜰 경우  \n","세션 다시 시작 클릭 -> 아래 셀을 한 번 더 실행"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63212,"status":"ok","timestamp":1733107386122,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"4xbF16Rv2N9o","outputId":"8382bb07-8686-4731-de05-b271da1957ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyyaml==5.3 in /usr/local/lib/python3.10/dist-packages (5.3)\n","Processing ./models/research\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: avro-python3 in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (1.10.2)\n","Requirement already satisfied: apache-beam in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.61.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (11.0.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (5.3.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (3.8.0)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (3.0.11)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (21.6.0)\n","Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (1.16.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.0.8)\n","Requirement already satisfied: lvis in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (0.5.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (1.13.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.2.2)\n","Requirement already satisfied: tf-models-official==2.8.0 in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.8.0)\n","Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (0.23.1)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.8.0)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.4.7)\n","Requirement already satisfied: sacrebleu<=2.2.0 in /usr/local/lib/python3.10/dist-packages (from object_detection==0.1) (2.2.0)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (0.5.0)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (2.151.0)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (1.6.17)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (1.26.4)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (4.1.3)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (4.10.0.84)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (5.9.5)\n","Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (9.0.0)\n","Requirement already satisfied: pyyaml<6.0,>=5.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (5.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (0.2.0)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (1.2.2)\n","Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (0.23.0)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (4.9.7)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (0.16.1)\n","Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (0.8.0)\n","Requirement already satisfied: tensorflow-text~=2.8.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (2.8.2)\n","Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.8.0->object_detection==0.1) (2.8.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->object_detection==0.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object_detection==0.1) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->object_detection==0.1) (2024.2)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object_detection==0.1) (3.0.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object_detection==0.1) (2024.9.11)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object_detection==0.1) (0.9.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object_detection==0.1) (0.4.6)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object_detection==0.1) (1.4.0)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (1.7)\n","Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (3.10.11)\n","Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.3.1.1)\n","Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (2.2.1)\n","Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (1.9.7)\n","Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.19)\n","Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (1.65.5)\n","Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (2.7.3)\n","Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.22.0)\n","Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (4.23.0)\n","Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (3.4.2)\n","Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.7.0)\n","Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (24.2)\n","Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (4.10.1)\n","Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (1.25.0)\n","Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (4.25.5)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (1.4.2)\n","Requirement already satisfied: redis<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (5.2.0)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (2.32.3)\n","Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (4.12.2)\n","Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.23.0)\n","Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object_detection==0.1) (0.6)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object_detection==0.1) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object_detection==0.1) (1.4.7)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object_detection==0.1) (4.10.0.84)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection==0.1) (1.3.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object_detection==0.1) (4.55.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem==0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object_detection==0.1) (0.23.1)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object_detection==0.1) (2.27.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object_detection==0.1) (0.2.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object_detection==0.1) (2.19.2)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.8.0->object_detection==0.1) (4.1.1)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object_detection==0.1) (0.6.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object_detection==0.1) (0.21.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object_detection==0.1) (2024.8.30)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object_detection==0.1) (4.66.6)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object_detection==0.1) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object_detection==0.1) (2.2.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.8.0->object_detection==0.1) (6.2.0)\n","Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam->object_detection==0.1) (2.7.0)\n","Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam->object_detection==0.1) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object_detection==0.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object_detection==0.1) (3.10)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (24.3.25)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (3.12.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (1.1.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (3.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (75.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (2.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (1.16.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (2.8.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (2.8.0)\n","Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.8.0->object_detection==0.1) (2.15.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.8.0->object_detection==0.1) (0.1.8)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object_detection==0.1) (0.6.1)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object_detection==0.1) (0.4.1)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.8.0->object_detection==0.1) (4.9)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.8.0->object_detection==0.1) (1.5.2)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->tf-models-official==2.8.0->object_detection==0.1) (2.13.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (8.1.7)\n","Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (4.2.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (2.3)\n","Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (0.1.6)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (1.13.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (0.10.2)\n","Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (0.5.1)\n","Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (1.10.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (0.45.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (2024.10.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (6.4.5)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (3.21.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object_detection==0.1) (1.66.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.8.0->object_detection==0.1) (5.5.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object_detection==0.1) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.8.0->object_detection==0.1) (3.5.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (3.1.3)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.8.0->object_detection==0.1) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.8.0->object_detection==0.1) (1.3)\n","Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->tf-models-official==2.8.0->object_detection==0.1) (0.16)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (3.0.2)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official==2.8.0->object_detection==0.1) (3.2.2)\n","Building wheels for collected packages: object_detection\n","  Building wheel for object_detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for object_detection: filename=object_detection-0.1-py3-none-any.whl size=1697355 sha256=98fe06999707faa13e25e5e175a2bf57b117b0cc148f04be97d48430a701ef60\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-f_vs31k9/wheels/53/dd/70/2de274d6c443c69d367bd6a5606f95e5a6df61aacf1435ec0d\n","Successfully built object_detection\n","Installing collected packages: object_detection\n","  Attempting uninstall: object_detection\n","    Found existing installation: object_detection 0.1\n","    Uninstalling object_detection-0.1:\n","      Successfully uninstalled object_detection-0.1\n","Successfully installed object_detection-0.1\n","Collecting tensorflow==2.8.0\n","  Using cached tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.12.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.4.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.25.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n","  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.23.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.65.5)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.45.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n","Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorflow\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.1\n","    Uninstalling tensorflow-2.8.1:\n","      Successfully uninstalled tensorflow-2.8.1\n","Successfully installed tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Requirement already satisfied: tensorflow_io==0.23.1 in /usr/local/lib/python3.10/dist-packages (0.23.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem==0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io==0.23.1) (0.23.1)\n"]}],"source":["# Object Detection API 설치 (참고: 이 코드 블록 실행에 약 10분 정도 소요될 수 있음)\n","\n","# PyYAML 라이브러리 관련 문제 해결\n","# Colab 환경에서 PyYAML v5.4.1을 설치할 수 없으므로 PyYAML v5.3 버전을 설치\n","!pip install pyyaml==5.3\n","\n","# Object Detection API를 설치\n","# 연구(research) 폴더에 있는 Python 패키지를 설치하여 사용할 준비\n","!pip install /content/models/research/\n","\n","# TensorFlow 버전을 2.8.0으로 다운그레이드\n","# Colab 환경에서 TensorFlow v2.10과의 호환성 문제(2022년 10월 기준)가 있어 v2.8.0으로 변경\n","!pip install tensorflow==2.8.0\n","\n","# TensorFlow v2.8.0과 호환되는 CUDA v11.0 설치\n","# TensorFlow v2.8.0은 CUDA v11.0과 호환되므로 이를 설치하여 GPU 가속을 지원\n","!pip install tensorflow_io==0.23.1  # TensorFlow와 호환되는 I/O 라이브러리 설치\n","\n","# # CUDA v11.0 관련 패키지를 다운로드하고 설치\n","# !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n","# !mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n","\n","# # CUDA v11.0 설치 파일을 다운로드\n","# !wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","\n","# # CUDA 설치 파일을 dpkg 명령어로 설치\n","# !dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","\n","# # CUDA의 공개 키를 추가하여 패키지를 인증\n","# !apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\n","\n","# # 시스템 패키지 업데이트 및 CUDA 툴킷 v11.0 설치\n","# !apt-get update && sudo apt-get install cuda-toolkit-11-0\n","\n","# # CUDA 라이브러리 경로를 환경 변수에 추가\n","# !export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH"]},{"cell_type":"markdown","metadata":{"id":"nv7g8Gyj3f-2"},"source":["- 아래 셀 실행 시 발생하는 오류는 무시"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":687},"executionInfo":{"elapsed":11187,"status":"ok","timestamp":1733107397304,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"753tBoqh3b9e","outputId":"0e7c39f8-357b-41ef-f982-ffe41e8e4fd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: protobuf 4.25.5\n","Uninstalling protobuf-4.25.5:\n","  Successfully uninstalled protobuf-4.25.5\n","Collecting protobuf==3.20.1\n","  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n","Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","apache-beam 2.61.0 requires protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n","google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-api-core 2.19.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-aiplatform 1.71.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-functions 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-iam 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-language 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-resource-manager 1.13.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n","grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.1 which is incompatible.\n","pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed protobuf-3.20.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"99d355da6aba4c12b0f92fad048a4a82"}},"metadata":{}}],"source":["# Protobuf 패키지 문제 해결을 위한 설치 과정\n","\n","!pip uninstall -y protobuf  # Protobuf 패키지를 제거 (-y 옵션으로 사용자 확인 없이 제거).\n","!pip install protobuf==3.20.1  # Protobuf 버전을 3.20.1로 다시 설치"]},{"cell_type":"markdown","metadata":{"id":"9eHNXn0C3rLH"},"source":["- 아래 셀을 통해 강제로 세션 종료  \n","그 다음 셀로 넘어가서 계속 진행하면 됨"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKP04Meu3pBc"},"outputs":[],"source":["import os\n","\n","# 현재 실행 중인 프로세스를 강제로 종료\n","os.kill(os.getpid(), 9)  # os.getpid()로 현재 프로세스 ID를 가져오고, 신호 9(SIGKILL)로 해당 프로세스를 종료"]},{"cell_type":"markdown","metadata":{"id":"eFdiIGP23xt6"},"source":["- 아래 셀 출력 값이 다음과 같다면 성공\n","```\n","Name: protobuf  \n","Version: 3.20.1\n","```\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3673,"status":"ok","timestamp":1733107422120,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"9atrEWW93w1Y","outputId":"ccf9266e-0fc3-4c48-b93c-6e400ded0cb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: protobuf\n","Version: 3.20.1\n","Summary: Protocol Buffers\n","Home-page: https://developers.google.com/protocol-buffers/\n","Author: \n","Author-email: \n","License: BSD-3-Clause\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: \n","Required-by: apache-beam, google-ai-generativelanguage, google-api-core, google-cloud-aiplatform, google-cloud-bigquery-connection, google-cloud-bigquery-storage, google-cloud-bigtable, google-cloud-datastore, google-cloud-firestore, google-cloud-functions, google-cloud-iam, google-cloud-language, google-cloud-pubsub, google-cloud-resource-manager, google-cloud-translate, google-generativeai, googleapis-common-protos, grpc-google-iam-v1, grpcio-status, orbax-checkpoint, proto-plus, tensorboard, tensorflow, tensorflow-datasets, tensorflow-hub, tensorflow-metadata, wandb\n"]}],"source":["# Protobuf 패키지 정보 확인\n","!pip show protobuf"]},{"cell_type":"markdown","metadata":{"id":"H_S3_tX48Rgj"},"source":["## 1-2. TensorFlow Object Detection API 설치 확인"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58019,"status":"ok","timestamp":1733107481867,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"tKB-ZxQY4KKv","outputId":"ac0d2cd0-276d-4f60-ba51-c6f7ddd94de1"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-12-02 02:43:44.188311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2024-12-02 02:43:44.188348: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","Exception ignored in: <function _get_module_lock.<locals>.cb at 0x7a4cb0283130>\n","Traceback (most recent call last):\n","  File \"<frozen importlib._bootstrap>\", line 207, in cb\n","KeyboardInterrupt: \n","2024-12-02 02:43:58.511883: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2024-12-02 02:43:58.511964: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2024-12-02 02:43:58.512009: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (77ccc732950b): /proc/driver/nvidia/version does not exist\n","Running tests under Python 3.10.12: /usr/bin/python3\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n","2024-12-02 02:43:58.550007: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","W1202 02:43:59.213916 134471470739456 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.58s\n","I1202 02:44:00.118152 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.58s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.43s\n","I1202 02:44:01.552165 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 1.43s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n","I1202 02:44:01.836304 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.28s\n","I1202 02:44:02.118662 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.28s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.21s\n","I1202 02:44:04.327311 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.21s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n","[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n","I1202 02:44:04.328755 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n","I1202 02:44:04.355758 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n","I1202 02:44:04.371653 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n","I1202 02:44:04.388114 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.11s\n","I1202 02:44:04.494231 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.11s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.1s\n","I1202 02:44:04.591387 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.1s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.11s\n","I1202 02:44:04.699151 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.11s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.1s\n","I1202 02:44:04.801426 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.1s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.1s\n","I1202 02:44:04.904900 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.1s\n","[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n","I1202 02:44:04.937423 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n","[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n","I1202 02:44:05.136480 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b0\n","I1202 02:44:05.136660 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 64\n","I1202 02:44:05.136755 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 3\n","I1202 02:44:05.139463 134471470739456 efficientnet_model.py:144] round_filter input=32 output=32\n","I1202 02:44:05.158766 134471470739456 efficientnet_model.py:144] round_filter input=32 output=32\n","I1202 02:44:05.158904 134471470739456 efficientnet_model.py:144] round_filter input=16 output=16\n","I1202 02:44:05.226037 134471470739456 efficientnet_model.py:144] round_filter input=16 output=16\n","I1202 02:44:05.226237 134471470739456 efficientnet_model.py:144] round_filter input=24 output=24\n","I1202 02:44:05.399449 134471470739456 efficientnet_model.py:144] round_filter input=24 output=24\n","I1202 02:44:05.399633 134471470739456 efficientnet_model.py:144] round_filter input=40 output=40\n","I1202 02:44:05.570424 134471470739456 efficientnet_model.py:144] round_filter input=40 output=40\n","I1202 02:44:05.570602 134471470739456 efficientnet_model.py:144] round_filter input=80 output=80\n","I1202 02:44:05.823740 134471470739456 efficientnet_model.py:144] round_filter input=80 output=80\n","I1202 02:44:05.823949 134471470739456 efficientnet_model.py:144] round_filter input=112 output=112\n","I1202 02:44:06.224662 134471470739456 efficientnet_model.py:144] round_filter input=112 output=112\n","I1202 02:44:06.224892 134471470739456 efficientnet_model.py:144] round_filter input=192 output=192\n","I1202 02:44:06.896422 134471470739456 efficientnet_model.py:144] round_filter input=192 output=192\n","I1202 02:44:06.896611 134471470739456 efficientnet_model.py:144] round_filter input=320 output=320\n","I1202 02:44:06.990566 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=1280\n","I1202 02:44:07.049923 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1202 02:44:07.108414 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b1\n","I1202 02:44:07.108604 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 88\n","I1202 02:44:07.108694 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 4\n","I1202 02:44:07.110475 134471470739456 efficientnet_model.py:144] round_filter input=32 output=32\n","I1202 02:44:07.131603 134471470739456 efficientnet_model.py:144] round_filter input=32 output=32\n","I1202 02:44:07.131767 134471470739456 efficientnet_model.py:144] round_filter input=16 output=16\n","I1202 02:44:07.279140 134471470739456 efficientnet_model.py:144] round_filter input=16 output=16\n","I1202 02:44:07.279321 134471470739456 efficientnet_model.py:144] round_filter input=24 output=24\n","I1202 02:44:07.527582 134471470739456 efficientnet_model.py:144] round_filter input=24 output=24\n","I1202 02:44:07.527764 134471470739456 efficientnet_model.py:144] round_filter input=40 output=40\n","I1202 02:44:07.783601 134471470739456 efficientnet_model.py:144] round_filter input=40 output=40\n","I1202 02:44:07.783790 134471470739456 efficientnet_model.py:144] round_filter input=80 output=80\n","I1202 02:44:08.139244 134471470739456 efficientnet_model.py:144] round_filter input=80 output=80\n","I1202 02:44:08.139435 134471470739456 efficientnet_model.py:144] round_filter input=112 output=112\n","I1202 02:44:08.744556 134471470739456 efficientnet_model.py:144] round_filter input=112 output=112\n","I1202 02:44:08.744754 134471470739456 efficientnet_model.py:144] round_filter input=192 output=192\n","I1202 02:44:09.434375 134471470739456 efficientnet_model.py:144] round_filter input=192 output=192\n","I1202 02:44:09.434612 134471470739456 efficientnet_model.py:144] round_filter input=320 output=320\n","I1202 02:44:09.765671 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=1280\n","I1202 02:44:09.850074 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1202 02:44:09.961707 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b2\n","I1202 02:44:09.961967 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 112\n","I1202 02:44:09.962084 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 5\n","I1202 02:44:09.964809 134471470739456 efficientnet_model.py:144] round_filter input=32 output=32\n","I1202 02:44:09.991343 134471470739456 efficientnet_model.py:144] round_filter input=32 output=32\n","I1202 02:44:09.991529 134471470739456 efficientnet_model.py:144] round_filter input=16 output=16\n","I1202 02:44:10.185371 134471470739456 efficientnet_model.py:144] round_filter input=16 output=16\n","I1202 02:44:10.185584 134471470739456 efficientnet_model.py:144] round_filter input=24 output=24\n","I1202 02:44:10.557245 134471470739456 efficientnet_model.py:144] round_filter input=24 output=24\n","I1202 02:44:10.557457 134471470739456 efficientnet_model.py:144] round_filter input=40 output=48\n","I1202 02:44:10.899805 134471470739456 efficientnet_model.py:144] round_filter input=40 output=48\n","I1202 02:44:10.900035 134471470739456 efficientnet_model.py:144] round_filter input=80 output=88\n","I1202 02:44:11.367525 134471470739456 efficientnet_model.py:144] round_filter input=80 output=88\n","I1202 02:44:11.367732 134471470739456 efficientnet_model.py:144] round_filter input=112 output=120\n","I1202 02:44:11.882100 134471470739456 efficientnet_model.py:144] round_filter input=112 output=120\n","I1202 02:44:11.882324 134471470739456 efficientnet_model.py:144] round_filter input=192 output=208\n","I1202 02:44:12.647239 134471470739456 efficientnet_model.py:144] round_filter input=192 output=208\n","I1202 02:44:12.647469 134471470739456 efficientnet_model.py:144] round_filter input=320 output=352\n","I1202 02:44:13.018148 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=1408\n","I1202 02:44:13.115521 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1202 02:44:13.232247 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b3\n","I1202 02:44:13.232459 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 160\n","I1202 02:44:13.232552 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 6\n","I1202 02:44:13.235206 134471470739456 efficientnet_model.py:144] round_filter input=32 output=40\n","I1202 02:44:13.260901 134471470739456 efficientnet_model.py:144] round_filter input=32 output=40\n","I1202 02:44:13.261095 134471470739456 efficientnet_model.py:144] round_filter input=16 output=24\n","I1202 02:44:13.452219 134471470739456 efficientnet_model.py:144] round_filter input=16 output=24\n","I1202 02:44:13.452454 134471470739456 efficientnet_model.py:144] round_filter input=24 output=32\n","I1202 02:44:13.792107 134471470739456 efficientnet_model.py:144] round_filter input=24 output=32\n","I1202 02:44:13.792285 134471470739456 efficientnet_model.py:144] round_filter input=40 output=48\n","I1202 02:44:14.037401 134471470739456 efficientnet_model.py:144] round_filter input=40 output=48\n","I1202 02:44:14.037587 134471470739456 efficientnet_model.py:144] round_filter input=80 output=96\n","I1202 02:44:14.470363 134471470739456 efficientnet_model.py:144] round_filter input=80 output=96\n","I1202 02:44:14.470569 134471470739456 efficientnet_model.py:144] round_filter input=112 output=136\n","I1202 02:44:14.916495 134471470739456 efficientnet_model.py:144] round_filter input=112 output=136\n","I1202 02:44:14.916682 134471470739456 efficientnet_model.py:144] round_filter input=192 output=232\n","I1202 02:44:15.540040 134471470739456 efficientnet_model.py:144] round_filter input=192 output=232\n","I1202 02:44:15.540262 134471470739456 efficientnet_model.py:144] round_filter input=320 output=384\n","I1202 02:44:15.801839 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=1536\n","I1202 02:44:15.863542 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1202 02:44:15.934896 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b4\n","I1202 02:44:15.935097 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 224\n","I1202 02:44:15.935179 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n","I1202 02:44:15.936913 134471470739456 efficientnet_model.py:144] round_filter input=32 output=48\n","I1202 02:44:15.953307 134471470739456 efficientnet_model.py:144] round_filter input=32 output=48\n","I1202 02:44:15.953432 134471470739456 efficientnet_model.py:144] round_filter input=16 output=24\n","I1202 02:44:16.404847 134471470739456 efficientnet_model.py:144] round_filter input=16 output=24\n","I1202 02:44:16.405049 134471470739456 efficientnet_model.py:144] round_filter input=24 output=32\n","I1202 02:44:16.759089 134471470739456 efficientnet_model.py:144] round_filter input=24 output=32\n","I1202 02:44:16.759274 134471470739456 efficientnet_model.py:144] round_filter input=40 output=56\n","I1202 02:44:17.107796 134471470739456 efficientnet_model.py:144] round_filter input=40 output=56\n","I1202 02:44:17.107998 134471470739456 efficientnet_model.py:144] round_filter input=80 output=112\n","I1202 02:44:17.651552 134471470739456 efficientnet_model.py:144] round_filter input=80 output=112\n","I1202 02:44:17.651734 134471470739456 efficientnet_model.py:144] round_filter input=112 output=160\n","I1202 02:44:18.212060 134471470739456 efficientnet_model.py:144] round_filter input=112 output=160\n","I1202 02:44:18.212248 134471470739456 efficientnet_model.py:144] round_filter input=192 output=272\n","I1202 02:44:19.102497 134471470739456 efficientnet_model.py:144] round_filter input=192 output=272\n","I1202 02:44:19.102692 134471470739456 efficientnet_model.py:144] round_filter input=320 output=448\n","I1202 02:44:19.389650 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=1792\n","I1202 02:44:19.463859 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1202 02:44:19.548497 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b5\n","I1202 02:44:19.548687 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 288\n","I1202 02:44:19.548775 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n","I1202 02:44:19.550550 134471470739456 efficientnet_model.py:144] round_filter input=32 output=48\n","I1202 02:44:19.569207 134471470739456 efficientnet_model.py:144] round_filter input=32 output=48\n","I1202 02:44:19.569339 134471470739456 efficientnet_model.py:144] round_filter input=16 output=24\n","I1202 02:44:19.783324 134471470739456 efficientnet_model.py:144] round_filter input=16 output=24\n","I1202 02:44:19.783507 134471470739456 efficientnet_model.py:144] round_filter input=24 output=40\n","I1202 02:44:20.222444 134471470739456 efficientnet_model.py:144] round_filter input=24 output=40\n","I1202 02:44:20.222635 134471470739456 efficientnet_model.py:144] round_filter input=40 output=64\n","I1202 02:44:21.175450 134471470739456 efficientnet_model.py:144] round_filter input=40 output=64\n","I1202 02:44:21.175689 134471470739456 efficientnet_model.py:144] round_filter input=80 output=128\n","I1202 02:44:22.964538 134471470739456 efficientnet_model.py:144] round_filter input=80 output=128\n","I1202 02:44:22.964733 134471470739456 efficientnet_model.py:144] round_filter input=112 output=176\n","I1202 02:44:23.631454 134471470739456 efficientnet_model.py:144] round_filter input=112 output=176\n","I1202 02:44:23.631644 134471470739456 efficientnet_model.py:144] round_filter input=192 output=304\n","I1202 02:44:25.248398 134471470739456 efficientnet_model.py:144] round_filter input=192 output=304\n","I1202 02:44:25.248632 134471470739456 efficientnet_model.py:144] round_filter input=320 output=512\n","I1202 02:44:26.041138 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=2048\n","I1202 02:44:26.152461 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1202 02:44:26.668382 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b6\n","I1202 02:44:26.668625 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n","I1202 02:44:26.668730 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n","I1202 02:44:26.683083 134471470739456 efficientnet_model.py:144] round_filter input=32 output=56\n","I1202 02:44:26.744693 134471470739456 efficientnet_model.py:144] round_filter input=32 output=56\n","I1202 02:44:26.744904 134471470739456 efficientnet_model.py:144] round_filter input=16 output=32\n","I1202 02:44:27.084346 134471470739456 efficientnet_model.py:144] round_filter input=16 output=32\n","I1202 02:44:27.084571 134471470739456 efficientnet_model.py:144] round_filter input=24 output=40\n","I1202 02:44:27.822186 134471470739456 efficientnet_model.py:144] round_filter input=24 output=40\n","I1202 02:44:27.822397 134471470739456 efficientnet_model.py:144] round_filter input=40 output=72\n","I1202 02:44:28.585905 134471470739456 efficientnet_model.py:144] round_filter input=40 output=72\n","I1202 02:44:28.586159 134471470739456 efficientnet_model.py:144] round_filter input=80 output=144\n","I1202 02:44:29.383305 134471470739456 efficientnet_model.py:144] round_filter input=80 output=144\n","I1202 02:44:29.383497 134471470739456 efficientnet_model.py:144] round_filter input=112 output=200\n","I1202 02:44:30.184649 134471470739456 efficientnet_model.py:144] round_filter input=112 output=200\n","I1202 02:44:30.184841 134471470739456 efficientnet_model.py:144] round_filter input=192 output=344\n","I1202 02:44:31.612915 134471470739456 efficientnet_model.py:144] round_filter input=192 output=344\n","I1202 02:44:31.613123 134471470739456 efficientnet_model.py:144] round_filter input=320 output=576\n","I1202 02:44:32.175977 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=2304\n","I1202 02:44:32.277667 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1202 02:44:32.390662 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b7\n","I1202 02:44:32.390860 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n","I1202 02:44:32.390963 134471470739456 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n","I1202 02:44:32.392748 134471470739456 efficientnet_model.py:144] round_filter input=32 output=64\n","I1202 02:44:32.410274 134471470739456 efficientnet_model.py:144] round_filter input=32 output=64\n","I1202 02:44:32.410403 134471470739456 efficientnet_model.py:144] round_filter input=16 output=32\n","I1202 02:44:32.723022 134471470739456 efficientnet_model.py:144] round_filter input=16 output=32\n","I1202 02:44:32.723199 134471470739456 efficientnet_model.py:144] round_filter input=24 output=48\n","I1202 02:44:33.357540 134471470739456 efficientnet_model.py:144] round_filter input=24 output=48\n","I1202 02:44:33.357760 134471470739456 efficientnet_model.py:144] round_filter input=40 output=80\n","I1202 02:44:33.967440 134471470739456 efficientnet_model.py:144] round_filter input=40 output=80\n","I1202 02:44:33.967633 134471470739456 efficientnet_model.py:144] round_filter input=80 output=160\n","I1202 02:44:35.312908 134471470739456 efficientnet_model.py:144] round_filter input=80 output=160\n","I1202 02:44:35.313152 134471470739456 efficientnet_model.py:144] round_filter input=112 output=224\n","I1202 02:44:36.390907 134471470739456 efficientnet_model.py:144] round_filter input=112 output=224\n","I1202 02:44:36.391121 134471470739456 efficientnet_model.py:144] round_filter input=192 output=384\n","I1202 02:44:38.174698 134471470739456 efficientnet_model.py:144] round_filter input=192 output=384\n","I1202 02:44:38.174901 134471470739456 efficientnet_model.py:144] round_filter input=320 output=640\n","I1202 02:44:39.290166 134471470739456 efficientnet_model.py:144] round_filter input=1280 output=2560\n","I1202 02:44:39.455412 134471470739456 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 34.74s\n","I1202 02:44:39.677726 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 34.74s\n","[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n","I1202 02:44:39.696244 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n","[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n","I1202 02:44:39.699000 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n","I1202 02:44:39.699706 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n","I1202 02:44:39.702037 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n","[ RUN      ] ModelBuilderTF2Test.test_session\n","[  SKIPPED ] ModelBuilderTF2Test.test_session\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n","I1202 02:44:39.704035 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n","I1202 02:44:39.704587 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n","I1202 02:44:39.706511 134471470739456 test_util.py:2373] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n","----------------------------------------------------------------------\n","Ran 24 tests in 41.179s\n","\n","OK (skipped=1)\n"]}],"source":["# 모델 빌더 테스트 파일 실행: 설정이 올바르게 작동하는지 확인하기 위한 테스트\n","\n","!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\n","# Object Detection API의 모델 빌더 구성 요소를 테스트하는 스크립트를 실행\n","# 이 스크립트는 설치 및 구성 파일들이 제대로 작동하는지 확인하는 데 사용됨"]},{"cell_type":"markdown","metadata":{"id":"wspwAJ904dNK"},"source":["# 2.&nbsp;데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"shYePDc68bFI"},"source":["## 2-1. 라벨 맵(labelmap.txt) 파일 생성\n","- 테스트 데이터의 형식을 참고하여 아래에 클래스 이름 작성하기"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qGOk09Bi4ezZ","executionInfo":{"status":"ok","timestamp":1733107481867,"user_tz":-540,"elapsed":4,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}}},"outputs":[],"source":["# 객체 탐지 모델이 탐지할 클래스 목록을 포함하는 \"labelmap.txt\" 파일 생성\n","%%bash\n","\n","# labelmap.txt 파일에 클래스 이름을 추가\n","cat <<EOF >> /content/labelmap.txt\n","motorcycle\n","bicycle\n","kickboard\n","EOF"]},{"cell_type":"markdown","metadata":{"id":"YL8xoDWY8jmU"},"source":["## 2-2. 데이터셋 위치 정의(train/test 데이터)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1802,"status":"ok","timestamp":1733107483665,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"VG1WeY0j4ie4","outputId":"15746835-5f4b-4c64-e494-2d144d8c4040"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","# Google Drive 연결\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Jl0ncbpaRBwS"},"source":["- 각 파일이 있는 경로 지정\n","  - `base_dir`\n","  - `train_record_fname`\n","  - `val_record_fname`\n","  - `label_map_pbtxt_fname`"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Uze6-74j4lTf","executionInfo":{"status":"ok","timestamp":1733107974244,"user_tz":-540,"elapsed":309,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}}},"outputs":[],"source":["# 작업 디렉토리 설정\n","base_dir = '/content/drive/MyDrive/data'\n","train_record_fname = f'{base_dir}/train/-.tfrecord'\n","val_record_fname = f'{base_dir}/valid/-.tfrecord'\n","label_map_pbtxt_fname = f'{base_dir}/train/-_label_map.pbtxt'"]},{"cell_type":"markdown","source":[],"metadata":{"id":"d0X16FlkSNu1"}},{"cell_type":"markdown","metadata":{"id":"ysI2ODW44qDb"},"source":["# 3.&nbsp;훈련 구성 설정"]},{"cell_type":"markdown","metadata":{"id":"Ozid90gD8rS-"},"source":["## 3-1. 사용할 모델 설정(SSD MobileNet V2)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"EHWwjhAB4pOL","executionInfo":{"status":"ok","timestamp":1733107975857,"user_tz":-540,"elapsed":394,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}}},"outputs":[],"source":["# TF2 Object Detection Zoo에서 제공하는 다양한 모델 중 하나를 선택하여 배포할 수 있도록 설정\n","\n","# 선택한 모델의 이름을 설정\n","chosen_model = 'ssd-mobilenet-v2-fpnlite-640'\n","\n","# 사용할 모델들의 구성 정보 딕셔너리\n","MODELS_CONFIG = {\n","    # 1. SSD MobileNet V2 (320x320 해상도, COCO 데이터셋 기반)\n","    'ssd-mobilenet-v2': {\n","        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',  # 모델 이름\n","        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',  # 기본 파이프라인 구성 파일\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',  # 사전 학습된 체크포인트 파일\n","    },\n","    # 2. EfficientDet-D0 (효율적인 작은 크기의 모델, COCO 데이터셋 기반)\n","    'efficientdet-d0': {\n","        'model_name': 'efficientdet_d0_coco17_tpu-32',\n","        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n","    },\n","    # 3. SSD MobileNet V2 FPNLite (320x320 해상도, COCO 데이터셋 기반, FPN 사용)\n","    'ssd-mobilenet-v2-fpnlite-320': {\n","        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n","        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n","    },\n","    # 4. CenterNet (현재 작동하지 않음 - 2022년 9월 10일 기준)\n","    # 'centernet-mobilenet-v2': {\n","    #     'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n","    #     'base_pipeline_file': 'pipeline.config',\n","    #     'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n","    # },\n","    # 1. SSD MobileNet V2 (640x640 해상도, COCO 데이터셋 기반)\n","    'ssd-mobilenet-v2': {\n","        'model_name': 'ssd_mobilenet_v2_640x640_coco17_tpu-8',  # 모델 이름\n","        'base_pipeline_file': 'ssd_mobilenet_v2_640x640_coco17_tpu-8.config',  # 기본 파이프라인 구성 파일\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_640x640_coco17_tpu-8.tar.gz',  # 사전 학습된 체크포인트 파일\n","    },\n","    # 2. EfficientDet-D0 (효율적인 작은 크기의 모델, COCO 데이터셋 기반)\n","    'efficientdet-d0': {\n","        'model_name': 'efficientdet_d0_coco17_tpu-32',\n","        'base_pipeline_file': 'ssd_efficientdet_d0_640x640_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n","    },\n","    # 3. SSD MobileNet V2 FPNLite (640x640 해상도, COCO 데이터셋 기반, FPN 사용)\n","    'ssd-mobilenet-v2-fpnlite-640': {\n","        'model_name': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8',\n","        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz',\n","    },\n","    # 4. CenterNet (현재 작동하지 않음 - 2022년 9월 10일 기준)\n","    # 'centernet-mobilenet-v2': {\n","    #     'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n","    #     'base_pipeline_file': 'pipeline.config',\n","    #     'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n","    # }\n","}\n","\n","# 선택된 모델에 해당하는 구성 정보 가져오기\n","model_name = MODELS_CONFIG[chosen_model]['model_name']  # 모델 이름\n","pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']  # 사전 학습된 체크포인트 경로\n","base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']  # 기본 파이프라인 구성 파일 경로"]},{"cell_type":"markdown","metadata":{"id":"mQRgaEYP8vq1"},"source":["## 3-2. 모델 구성 파일 및 사전 학습된 가중치 다운로드"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":857,"status":"ok","timestamp":1733107976714,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"tv6L8xOV4yOp","outputId":"00ac94dd-3020-44f4-a74d-11ca1419d89a"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/models/mymodel/’: File exists\n","/content/models/mymodel\n","--2024-12-02 02:52:55--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n","Resolving download.tensorflow.org (download.tensorflow.org)... 172.253.63.207, 142.250.31.207, 142.251.111.207, ...\n","Connecting to download.tensorflow.org (download.tensorflow.org)|172.253.63.207|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20518283 (20M) [application/x-tar]\n","Saving to: ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz.1’\n","\n","ssd_mobilenet_v2_fp 100%[===================>]  19.57M  --.-KB/s    in 0.08s   \n","\n","2024-12-02 02:52:56 (244 MB/s) - ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz.1’ saved [20518283/20518283]\n","\n","--2024-12-02 02:52:56--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4684 (4.6K) [text/plain]\n","Saving to: ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config.1’\n","\n","ssd_mobilenet_v2_fp 100%[===================>]   4.57K  --.-KB/s    in 0s      \n","\n","2024-12-02 02:52:56 (39.6 MB/s) - ‘ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config.1’ saved [4684/4684]\n","\n"]}],"source":["# \"mymodel\" 폴더 생성: 사전 학습된 가중치와 구성 파일을 저장할 디렉토리 생성\n","%mkdir /content/models/mymodel/\n","%cd /content/models/mymodel/\n","\n","# 사전 학습된 모델 가중치 다운로드\n","import tarfile  # tar 파일 압축 해제를 위한 라이브러리\n","download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n","\n","# 설정된 URL에서 가중치 파일 다운로드\n","!wget {download_tar}\n","\n","tar = tarfile.open(pretrained_checkpoint)  # 다운로드한 tar 파일 열기\n","tar.extractall()  # tar 파일의 모든 내용을 현재 디렉토리에 추출\n","tar.close()  # 파일 닫기\n","\n","# 학습 구성 파일 다운로드\n","# 해당 모델의 훈련을 위한 기본 설정이 포함된 구성 파일(pipeline.config)을 다운로드\n","download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n","\n","# 구성 파일을 지정된 URL에서 다운로드\n","!wget {download_config}"]},{"cell_type":"markdown","metadata":{"id":"MRm3ThfJ8y_I"},"source":["## 3-3. 훈련 단계 및 배치 크기 설정\n","$$\n","\\text{Number of epochs} = \\frac{\\text{num steps} \\times \\text{batch size}}{\\text{number of data}}\n","$$\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"HhbyCQib41AV","executionInfo":{"status":"ok","timestamp":1733107978189,"user_tz":-540,"elapsed":345,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}}},"outputs":[],"source":["# 모델 훈련을 위한 파라미터 설정\n","num_steps = 125000  # 총 훈련 단계 수 설정 (훈련 반복 횟수)\n","\n","# 선택한 모델에 따라 배치 크기(batch size) 설정\n","if chosen_model == 'efficientdet-d0':  # EfficientDet-D0 모델인 경우\n","  batch_size = 4  # 작은 모델이므로 더 작은 배치 크기 사용\n","else:\n","  batch_size = 16"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1733107978714,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"O-4mZ65K44az","outputId":"66f50ae6-4033-4e65-9705-a79886515003"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total classes: 3\n"]}],"source":["# 파일 경로 및 클래스 수 가져오기\n","# 모델 훈련에 필요한 설정 파일 및 체크포인트 파일 경로 설정\n","pipeline_fname = '/content/models/mymodel/' + base_pipeline_file  # 파이프라인 구성 파일 경로\n","fine_tune_checkpoint = '/content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite/ckpt-11.data-00000-of-00001'  # 사전 학습된 체크포인트 파일 경로\n","\n","# 라벨 맵에서 클래스 수를 계산하는 함수 정의\n","def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util  # Object Detection API에서 제공하는 유틸리티 모듈\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)  # 라벨 맵 파일 로드\n","    categories = label_map_util.convert_label_map_to_categories(  # 라벨 맵 데이터를 카테고리로 변환\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)  # 카테고리 인덱스를 생성\n","    return len(category_index.keys())  # 카테고리(클래스) 수 반환\n","\n","# 라벨 맵 파일에서 총 클래스 수 계산\n","num_classes = get_num_classes(label_map_pbtxt_fname)  # 라벨 맵 파일 경로를 전달하여 클래스 수 가져오기\n","print('Total classes:', num_classes)  # 클래스 수 출력"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1759,"status":"ok","timestamp":1733107980826,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"MV9u2WRR45ti","outputId":"52ae1178-2abd-4b7c-9270-c3d6ba54612f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# 구글 드라이브 연결 확인\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"_hntoks69BGh"},"source":["# 4.&nbsp;사용자 정의 구성 파일 작성"]},{"cell_type":"markdown","metadata":{"id":"YOfPW2tcBnh8"},"source":["## 4-1. 데이터셋 경로, 라벨 맵 경로, 가중치 경로 등 업데이트"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1733107985862,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"qjJv2DC_48AO","outputId":"d104a61d-490d-4d9b-c1bf-c8c8f1c0e674"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/models/mymodel\n","writing custom configuration file\n"]}],"source":["# 기본 파이프라인 파일을 수정하여 사용자 정의 구성 파일 생성\n","# 데이터셋, 모델 체크포인트, 학습 파라미터를 작성\n","\n","import re\n","\n","# 작업 디렉토리 변경\n","%cd /content/models/mymodel\n","print('writing custom configuration file')\n","\n","# 기존 파이프라인 파일 읽기\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","\n","# 수정된 내용을 새로운 구성 파일에 쓰기\n","with open('pipeline_file.config', 'w') as f:\n","\n","    # 사전 학습된 체크포인트 경로 설정\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","\n","    # 학습 데이터(train) 및 검증 데이터(val) 경로 설정\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n","\n","    # 라벨 맵 경로 설정\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # 배치 크기 설정\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # 학습 단계 수(num_steps) 설정\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","\n","    # 클래스 수(num_classes) 설정\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","\n","    # 체크포인트 유형을 \"classification\"에서 \"detection\"으로 변경\n","    s = re.sub(\n","        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n","\n","    # SSD MobileNet V2 모델의 경우 학습률(learning rate) 감소\n","    if chosen_model == 'ssd-mobilenet-v2':\n","      s = re.sub('learning_rate_base: .8',\n","                 'learning_rate_base: .08', s)\n","      s = re.sub('warmup_learning_rate: 0.13333',\n","                 'warmup_learning_rate: .026666', s)\n","\n","    # EfficientDet-D0 모델의 경우, TFLite와 호환되지 않는 구성을 수정\n","    if chosen_model == 'efficientdet-d0':\n","      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)  # 비율 유지 대신 고정 크기 사용\n","      s = re.sub('pad_to_max_dimension: true', '', s)  # 패딩 제거\n","      s = re.sub('min_dimension', 'height', s)  # 최소 크기를 높이로 변경\n","      s = re.sub('max_dimension', 'width', s)  # 최대 크기를 너비로 변경\n","\n","    # 수정된 내용을 파일에 작성\n","    f.write(s)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1733107989793,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"Sr6EwDZ54-yU","outputId":"8f141c81-75f0-4e93-ed11-a86e20ae1afd"},"outputs":[{"output_type":"stream","name":"stdout","text":["# SSD with Mobilenet v2 FPN-lite (go/fpn-lite) feature extractor, shared box\n","# predictor and focal loss (a mobile version of Retinanet).\n","# Retinanet: see Lin et al, https://arxiv.org/abs/1708.02002\n","# Trained on COCO, initialized from Imagenet classification checkpoint\n","# Train on TPU-8\n","#\n","# Achieves 28.2 mAP on COCO17 Val\n","\n","model {\n","  ssd {\n","    inplace_batchnorm_update: true\n","    freeze_batchnorm: false\n","    num_classes: 3\n","    box_coder {\n","      faster_rcnn_box_coder {\n","        y_scale: 10.0\n","        x_scale: 10.0\n","        height_scale: 5.0\n","        width_scale: 5.0\n","      }\n","    }\n","    matcher {\n","      argmax_matcher {\n","        matched_threshold: 0.5\n","        unmatched_threshold: 0.5\n","        ignore_thresholds: false\n","        negatives_lower_than_unmatched: true\n","        force_match_for_each_row: true\n","        use_matmul_gather: true\n","      }\n","    }\n","    similarity_calculator {\n","      iou_similarity {\n","      }\n","    }\n","    encode_background_as_zeros: true\n","    anchor_generator {\n","      multiscale_anchor_generator {\n","        min_level: 3\n","        max_level: 7\n","        anchor_scale: 4.0\n","        aspect_ratios: [1.0, 2.0, 0.5]\n","        scales_per_octave: 2\n","      }\n","    }\n","    image_resizer {\n","      fixed_shape_resizer {\n","        height: 640\n","        width: 640\n","      }\n","    }\n","    box_predictor {\n","      weight_shared_convolutional_box_predictor {\n","        depth: 128\n","        class_prediction_bias_init: -4.6\n","        conv_hyperparams {\n","          activation: RELU_6,\n","          regularizer {\n","            l2_regularizer {\n","              weight: 0.00004\n","            }\n","          }\n","          initializer {\n","            random_normal_initializer {\n","              stddev: 0.01\n","              mean: 0.0\n","            }\n","          }\n","          batch_norm {\n","            scale: true,\n","            decay: 0.997,\n","            epsilon: 0.001,\n","          }\n","        }\n","        num_layers_before_predictor: 4\n","        share_prediction_tower: true\n","        use_depthwise: true\n","        kernel_size: 3\n","      }\n","    }\n","    feature_extractor {\n","      type: 'ssd_mobilenet_v2_fpn_keras'\n","      use_depthwise: true\n","      fpn {\n","        min_level: 3\n","        max_level: 7\n","        additional_layer_depth: 128\n","      }\n","      min_depth: 16\n","      depth_multiplier: 1.0\n","      conv_hyperparams {\n","        activation: RELU_6,\n","        regularizer {\n","          l2_regularizer {\n","            weight: 0.00004\n","          }\n","        }\n","        initializer {\n","          random_normal_initializer {\n","            stddev: 0.01\n","            mean: 0.0\n","          }\n","        }\n","        batch_norm {\n","          scale: true,\n","          decay: 0.997,\n","          epsilon: 0.001,\n","        }\n","      }\n","      override_base_feature_extractor_hyperparams: true\n","    }\n","    loss {\n","      classification_loss {\n","        weighted_sigmoid_focal {\n","          alpha: 0.25\n","          gamma: 2.0\n","        }\n","      }\n","      localization_loss {\n","        weighted_smooth_l1 {\n","        }\n","      }\n","      classification_weight: 1.0\n","      localization_weight: 1.0\n","    }\n","    normalize_loss_by_num_matches: true\n","    normalize_loc_loss_by_codesize: true\n","    post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 1e-8\n","        iou_threshold: 0.6\n","        max_detections_per_class: 100\n","        max_total_detections: 100\n","      }\n","      score_converter: SIGMOID\n","    }\n","  }\n","}\n","\n","train_config: {\n","  fine_tune_checkpoint_version: V2\n","  fine_tune_checkpoint: \"/content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite/ckpt-11.data-00000-of-00001\"\n","  fine_tune_checkpoint_type: \"detection\"\n","  batch_size: 16\n","  sync_replicas: true\n","  startup_delay_steps: 0\n","  replicas_to_aggregate: 8\n","  num_steps: 125000\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","  data_augmentation_options {\n","    random_crop_image {\n","      min_object_covered: 0.0\n","      min_aspect_ratio: 0.75\n","      max_aspect_ratio: 3.0\n","      min_area: 0.75\n","      max_area: 1.0\n","      overlap_thresh: 0.0\n","    }\n","  }\n","  optimizer {\n","    momentum_optimizer: {\n","      learning_rate: {\n","        cosine_decay_learning_rate {\n","          learning_rate_base: .08\n","          total_steps: 50000\n","          warmup_learning_rate: .026666\n","          warmup_steps: 1000\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","    }\n","    use_moving_average: false\n","  }\n","  max_number_of_boxes: 100\n","  unpad_groundtruth_tensors: false\n","}\n","\n","train_input_reader: {\n","  label_map_path: \"/content/drive/MyDrive/data/train/-_label_map.pbtxt\"\n","  tf_record_input_reader {\n","    input_path: \"/content/drive/MyDrive/data/train/-.tfrecord\"\n","  }\n","}\n","\n","eval_config: {\n","  metrics_set: \"coco_detection_metrics\"\n","  use_moving_averages: false\n","}\n","\n","eval_input_reader: {\n","  label_map_path: \"/content/drive/MyDrive/data/train/-_label_map.pbtxt\"\n","  shuffle: false\n","  num_epochs: 1\n","  tf_record_input_reader {\n","    input_path: \"/content/drive/MyDrive/data/valid/-.tfrecord\"\n","  }\n","}\n","\n"]}],"source":["# (Optional) 사용자 정의 구성 파일의 내용을 출력하여 확인\n","\n","!cat /content/models/mymodel/pipeline_file.config\n","# 터미널에서 pipeline_file.config 파일의 내용을 출력\n","# 작성된 구성 파일이 올바른지 확인하는 데 사용"]},{"cell_type":"markdown","metadata":{"id":"oacZ6OPQBsOa"},"source":["## 4-2. 사용자 정의 파일 경로 설정"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"mf4bpWsu5CFx","executionInfo":{"status":"ok","timestamp":1733108042743,"user_tz":-540,"elapsed":239,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}}},"outputs":[],"source":["# 사용자 정의 구성 파일 경로 및 훈련 체크포인트 저장 디렉토리 설정\n","\n","pipeline_file = '/content/models/mymodel/pipeline_file.config'  # 사용자 정의 구성 파일 경로\n","model_dir = '/content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite'  # 훈련 중 생성되는 체크포인트 파일을 저장할 디렉토리 경로"]},{"cell_type":"markdown","metadata":{"id":"U0Dq9W6y5DeR"},"source":["# 5.&nbsp;모델 훈련"]},{"cell_type":"markdown","metadata":{"id":"Gp2EHwvYB0R0"},"source":["## 5-1. TensorBoard를 활용한 훈련 모니터링"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2917,"status":"ok","timestamp":1733108046439,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"L2DxZfdX5DNZ","outputId":"822dca12-7e7b-415a-a90c-c42c72f807bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.8.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.65.5)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.32.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.45.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"]}],"source":["!pip install tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4194,"status":"ok","timestamp":1732670631575,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"},"user_tz":-540},"id":"uvbAqjXz5JGr","outputId":"81e50ce5-62aa-41c9-fbe3-d864c073f521"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: tensorboard\n","Version: 2.8.0\n","Summary: TensorBoard lets you watch Tensors Flow\n","Home-page: https://github.com/tensorflow/tensorboard\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, google-auth, google-auth-oauthlib, grpcio, markdown, numpy, protobuf, requests, setuptools, tensorboard-data-server, tensorboard-plugin-wit, werkzeug, wheel\n","Required-by: tensorflow\n"]}],"source":["!pip show tensorboard"]},{"cell_type":"markdown","metadata":{"id":"V4hdnGLEZ9qM"},"source":["- 아래 코드는 일단 실행  \n","but 처음엔 아무 내용도 나오지 않음  \n","모델 학습을 진행하는 중에 새로고침하면 학습 과정 그래프 볼 수 있음"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rt2eqOey5Loi"},"outputs":[],"source":["# TensorBoard 확장 로드 및 로그 디렉토리 설정\n","\n","# Jupyter/Colab 환경에서 TensorBoard 확장 로드\n","%load_ext tensorboard\n","\n","# TensorBoard를 실행하여 '/content/training/train' 디렉토리의 로그 데이터 시각화\n","%tensorboard --logdir '/content/training/train'"]},{"cell_type":"markdown","metadata":{"id":"39MsT_ffB3vn"},"source":["## 5-2. Object Detection API를 이용한 훈련 실행"]},{"cell_type":"markdown","metadata":{"id":"dQLP26Hiax2p"},"source":["\n","\n","```\n","!python /content/models/research/object_detection/model_main_tf2.py \\  # Object Detection API의 주요 훈련 스크립트 실행\n","    --pipeline_config_path={pipeline_file} \\  # 사용자 정의 구성 파일의 경로를 전달\n","    --model_dir={model_dir} \\  # 체크포인트 파일과 로그를 저장할 디렉토리 경로 지정\n","    --alsologtostderr \\  # 표준 오류(stdout)로도 로그 메시지를 출력하도록 설정\n","    --num_train_steps={num_steps} \\  # 훈련 단계 수를 설정 (여기서는 {num_steps}로 지정)\n","    --sample_1_of_n_eval_examples=1  # 평가 데이터에서 1/n 샘플을 사용하여 평가를 수행 (여기서는 모든 예제를 평가)\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1785418,"status":"ok","timestamp":1732672441292,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"},"user_tz":-540},"id":"4z0-GgCV5PBT","outputId":"9caea5cd-5354-4e7a-edc7-fff47839c873"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n","2024-11-27 01:24:20.435099: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","I1127 01:24:20.444624 135971017113600 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","INFO:tensorflow:Maybe overwriting train_steps: 10000\n","I1127 01:24:20.448112 135971017113600 config_util.py:552] Maybe overwriting train_steps: 10000\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I1127 01:24:20.448268 135971017113600 config_util.py:552] Maybe overwriting use_bfloat16: False\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","W1127 01:24:20.478807 135971017113600 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","INFO:tensorflow:Reading unweighted datasets: ['/content/drive/MyDrive/nipa_google/data/506_crossmate.v2i.tfrecord/train/bicycle-3ssc.tfrecord']\n","I1127 01:24:20.501138 135971017113600 dataset_builder.py:162] Reading unweighted datasets: ['/content/drive/MyDrive/nipa_google/data/506_crossmate.v2i.tfrecord/train/bicycle-3ssc.tfrecord']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/drive/MyDrive/nipa_google/data/506_crossmate.v2i.tfrecord/train/bicycle-3ssc.tfrecord']\n","I1127 01:24:20.501763 135971017113600 dataset_builder.py:79] Reading record datasets for input file: ['/content/drive/MyDrive/nipa_google/data/506_crossmate.v2i.tfrecord/train/bicycle-3ssc.tfrecord']\n","INFO:tensorflow:Number of filenames to read: 1\n","I1127 01:24:20.501900 135971017113600 dataset_builder.py:80] Number of filenames to read: 1\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W1127 01:24:20.501986 135971017113600 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n","W1127 01:24:20.508149 135971017113600 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W1127 01:24:20.672097 135971017113600 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W1127 01:24:28.889170 135971017113600 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","W1127 01:24:31.537625 135971017113600 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W1127 01:24:32.895670 135971017113600 deprecation.py:337] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","/usr/local/lib/python3.10/dist-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n","  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n","I1127 01:24:46.257836 135965284714048 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n","I1127 01:24:57.623113 135965284714048 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n","2024-11-27 01:25:04.004313: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n","\n","You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.694375 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.696362 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.699398 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.700664 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.703507 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.704717 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.708643 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.709810 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.712689 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1127 01:25:08.713868 135971017113600 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","W1127 01:25:09.565941 135965301499456 deprecation.py:541] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","I1127 01:25:11.320558 135965301499456 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n","I1127 01:25:19.846276 135965301499456 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n","I1127 01:25:26.688696 135965301499456 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n","I1127 01:25:33.898964 135965301499456 api.py:441] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n","INFO:tensorflow:Step 100 per-step time 0.564s\n","I1127 01:26:05.425282 135971017113600 model_lib_v2.py:705] Step 100 per-step time 0.564s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7533354,\n"," 'Loss/localization_loss': 0.67090493,\n"," 'Loss/regularization_loss': 0.15421675,\n"," 'Loss/total_loss': 1.5784571,\n"," 'learning_rate': 0.0319994}\n","I1127 01:26:05.425655 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.7533354,\n"," 'Loss/localization_loss': 0.67090493,\n"," 'Loss/regularization_loss': 0.15421675,\n"," 'Loss/total_loss': 1.5784571,\n"," 'learning_rate': 0.0319994}\n","INFO:tensorflow:Step 200 per-step time 0.158s\n","I1127 01:26:21.197240 135971017113600 model_lib_v2.py:705] Step 200 per-step time 0.158s\n","INFO:tensorflow:{'Loss/classification_loss': 0.5676071,\n"," 'Loss/localization_loss': 0.62484086,\n"," 'Loss/regularization_loss': 0.15432848,\n"," 'Loss/total_loss': 1.3467764,\n"," 'learning_rate': 0.0373328}\n","I1127 01:26:21.197584 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.5676071,\n"," 'Loss/localization_loss': 0.62484086,\n"," 'Loss/regularization_loss': 0.15432848,\n"," 'Loss/total_loss': 1.3467764,\n"," 'learning_rate': 0.0373328}\n","INFO:tensorflow:Step 300 per-step time 0.160s\n","I1127 01:26:37.184167 135971017113600 model_lib_v2.py:705] Step 300 per-step time 0.160s\n","INFO:tensorflow:{'Loss/classification_loss': 0.5065322,\n"," 'Loss/localization_loss': 0.5989756,\n"," 'Loss/regularization_loss': 0.1544877,\n"," 'Loss/total_loss': 1.2599956,\n"," 'learning_rate': 0.0426662}\n","I1127 01:26:37.184495 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.5065322,\n"," 'Loss/localization_loss': 0.5989756,\n"," 'Loss/regularization_loss': 0.1544877,\n"," 'Loss/total_loss': 1.2599956,\n"," 'learning_rate': 0.0426662}\n","INFO:tensorflow:Step 400 per-step time 0.177s\n","I1127 01:26:54.965742 135971017113600 model_lib_v2.py:705] Step 400 per-step time 0.177s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6976737,\n"," 'Loss/localization_loss': 0.5781186,\n"," 'Loss/regularization_loss': 0.15476204,\n"," 'Loss/total_loss': 1.4305544,\n"," 'learning_rate': 0.047999598}\n","I1127 01:26:54.966089 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.6976737,\n"," 'Loss/localization_loss': 0.5781186,\n"," 'Loss/regularization_loss': 0.15476204,\n"," 'Loss/total_loss': 1.4305544,\n"," 'learning_rate': 0.047999598}\n","INFO:tensorflow:Step 500 per-step time 0.182s\n","I1127 01:27:13.137894 135971017113600 model_lib_v2.py:705] Step 500 per-step time 0.182s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7289196,\n"," 'Loss/localization_loss': 0.64403224,\n"," 'Loss/regularization_loss': 0.15502368,\n"," 'Loss/total_loss': 1.5279756,\n"," 'learning_rate': 0.053333}\n","I1127 01:27:13.138297 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.7289196,\n"," 'Loss/localization_loss': 0.64403224,\n"," 'Loss/regularization_loss': 0.15502368,\n"," 'Loss/total_loss': 1.5279756,\n"," 'learning_rate': 0.053333}\n","INFO:tensorflow:Step 600 per-step time 0.174s\n","I1127 01:27:30.567187 135971017113600 model_lib_v2.py:705] Step 600 per-step time 0.174s\n","INFO:tensorflow:{'Loss/classification_loss': 0.8103765,\n"," 'Loss/localization_loss': 0.5740774,\n"," 'Loss/regularization_loss': 0.15528008,\n"," 'Loss/total_loss': 1.5397341,\n"," 'learning_rate': 0.0586664}\n","I1127 01:27:30.567561 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.8103765,\n"," 'Loss/localization_loss': 0.5740774,\n"," 'Loss/regularization_loss': 0.15528008,\n"," 'Loss/total_loss': 1.5397341,\n"," 'learning_rate': 0.0586664}\n","INFO:tensorflow:Step 700 per-step time 0.170s\n","I1127 01:27:47.534718 135971017113600 model_lib_v2.py:705] Step 700 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.61316985,\n"," 'Loss/localization_loss': 0.5646662,\n"," 'Loss/regularization_loss': 0.15554169,\n"," 'Loss/total_loss': 1.3333777,\n"," 'learning_rate': 0.0639998}\n","I1127 01:27:47.535032 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.61316985,\n"," 'Loss/localization_loss': 0.5646662,\n"," 'Loss/regularization_loss': 0.15554169,\n"," 'Loss/total_loss': 1.3333777,\n"," 'learning_rate': 0.0639998}\n","INFO:tensorflow:Step 800 per-step time 0.165s\n","I1127 01:28:04.038561 135971017113600 model_lib_v2.py:705] Step 800 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.7172655,\n"," 'Loss/localization_loss': 0.6445819,\n"," 'Loss/regularization_loss': 0.15590474,\n"," 'Loss/total_loss': 1.5177522,\n"," 'learning_rate': 0.069333196}\n","I1127 01:28:04.038864 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.7172655,\n"," 'Loss/localization_loss': 0.6445819,\n"," 'Loss/regularization_loss': 0.15590474,\n"," 'Loss/total_loss': 1.5177522,\n"," 'learning_rate': 0.069333196}\n","INFO:tensorflow:Step 900 per-step time 0.184s\n","I1127 01:28:22.491862 135971017113600 model_lib_v2.py:705] Step 900 per-step time 0.184s\n","INFO:tensorflow:{'Loss/classification_loss': 0.44822142,\n"," 'Loss/localization_loss': 0.45240092,\n"," 'Loss/regularization_loss': 0.15633705,\n"," 'Loss/total_loss': 1.0569594,\n"," 'learning_rate': 0.074666604}\n","I1127 01:28:22.492230 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.44822142,\n"," 'Loss/localization_loss': 0.45240092,\n"," 'Loss/regularization_loss': 0.15633705,\n"," 'Loss/total_loss': 1.0569594,\n"," 'learning_rate': 0.074666604}\n","INFO:tensorflow:Step 1000 per-step time 0.177s\n","I1127 01:28:40.139649 135971017113600 model_lib_v2.py:705] Step 1000 per-step time 0.177s\n","INFO:tensorflow:{'Loss/classification_loss': 0.71140355,\n"," 'Loss/localization_loss': 0.5617614,\n"," 'Loss/regularization_loss': 0.156884,\n"," 'Loss/total_loss': 1.430049,\n"," 'learning_rate': 0.08}\n","I1127 01:28:40.139955 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.71140355,\n"," 'Loss/localization_loss': 0.5617614,\n"," 'Loss/regularization_loss': 0.156884,\n"," 'Loss/total_loss': 1.430049,\n"," 'learning_rate': 0.08}\n","INFO:tensorflow:Step 1100 per-step time 0.181s\n","I1127 01:28:58.249001 135971017113600 model_lib_v2.py:705] Step 1100 per-step time 0.181s\n","INFO:tensorflow:{'Loss/classification_loss': 0.62577826,\n"," 'Loss/localization_loss': 0.48363337,\n"," 'Loss/regularization_loss': 0.15746745,\n"," 'Loss/total_loss': 1.2668791,\n"," 'learning_rate': 0.07999918}\n","I1127 01:28:58.249397 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.62577826,\n"," 'Loss/localization_loss': 0.48363337,\n"," 'Loss/regularization_loss': 0.15746745,\n"," 'Loss/total_loss': 1.2668791,\n"," 'learning_rate': 0.07999918}\n","INFO:tensorflow:Step 1200 per-step time 0.174s\n","I1127 01:29:15.618179 135971017113600 model_lib_v2.py:705] Step 1200 per-step time 0.174s\n","INFO:tensorflow:{'Loss/classification_loss': 0.668049,\n"," 'Loss/localization_loss': 0.5266692,\n"," 'Loss/regularization_loss': 0.15806046,\n"," 'Loss/total_loss': 1.3527786,\n"," 'learning_rate': 0.079996705}\n","I1127 01:29:15.618596 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.668049,\n"," 'Loss/localization_loss': 0.5266692,\n"," 'Loss/regularization_loss': 0.15806046,\n"," 'Loss/total_loss': 1.3527786,\n"," 'learning_rate': 0.079996705}\n","INFO:tensorflow:Step 1300 per-step time 0.168s\n","I1127 01:29:32.409054 135971017113600 model_lib_v2.py:705] Step 1300 per-step time 0.168s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6158093,\n"," 'Loss/localization_loss': 0.5510532,\n"," 'Loss/regularization_loss': 0.15825173,\n"," 'Loss/total_loss': 1.3251143,\n"," 'learning_rate': 0.0799926}\n","I1127 01:29:32.409381 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.6158093,\n"," 'Loss/localization_loss': 0.5510532,\n"," 'Loss/regularization_loss': 0.15825173,\n"," 'Loss/total_loss': 1.3251143,\n"," 'learning_rate': 0.0799926}\n","INFO:tensorflow:Step 1400 per-step time 0.167s\n","I1127 01:29:49.109585 135971017113600 model_lib_v2.py:705] Step 1400 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.5941662,\n"," 'Loss/localization_loss': 0.47627416,\n"," 'Loss/regularization_loss': 0.15854609,\n"," 'Loss/total_loss': 1.2289865,\n"," 'learning_rate': 0.07998685}\n","I1127 01:29:49.109926 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.5941662,\n"," 'Loss/localization_loss': 0.47627416,\n"," 'Loss/regularization_loss': 0.15854609,\n"," 'Loss/total_loss': 1.2289865,\n"," 'learning_rate': 0.07998685}\n","INFO:tensorflow:Step 1500 per-step time 0.191s\n","I1127 01:30:08.212538 135971017113600 model_lib_v2.py:705] Step 1500 per-step time 0.191s\n","INFO:tensorflow:{'Loss/classification_loss': 0.56523925,\n"," 'Loss/localization_loss': 0.48981106,\n"," 'Loss/regularization_loss': 0.15901008,\n"," 'Loss/total_loss': 1.2140604,\n"," 'learning_rate': 0.07997945}\n","I1127 01:30:08.212904 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.56523925,\n"," 'Loss/localization_loss': 0.48981106,\n"," 'Loss/regularization_loss': 0.15901008,\n"," 'Loss/total_loss': 1.2140604,\n"," 'learning_rate': 0.07997945}\n","INFO:tensorflow:Step 1600 per-step time 0.181s\n","I1127 01:30:26.249285 135971017113600 model_lib_v2.py:705] Step 1600 per-step time 0.181s\n","INFO:tensorflow:{'Loss/classification_loss': 0.48564017,\n"," 'Loss/localization_loss': 0.3746755,\n"," 'Loss/regularization_loss': 0.15931226,\n"," 'Loss/total_loss': 1.0196279,\n"," 'learning_rate': 0.079970405}\n","I1127 01:30:26.249602 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.48564017,\n"," 'Loss/localization_loss': 0.3746755,\n"," 'Loss/regularization_loss': 0.15931226,\n"," 'Loss/total_loss': 1.0196279,\n"," 'learning_rate': 0.079970405}\n","INFO:tensorflow:Step 1700 per-step time 0.166s\n","I1127 01:30:42.866586 135971017113600 model_lib_v2.py:705] Step 1700 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.5160707,\n"," 'Loss/localization_loss': 0.48524344,\n"," 'Loss/regularization_loss': 0.15960306,\n"," 'Loss/total_loss': 1.1609173,\n"," 'learning_rate': 0.07995972}\n","I1127 01:30:42.866894 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.5160707,\n"," 'Loss/localization_loss': 0.48524344,\n"," 'Loss/regularization_loss': 0.15960306,\n"," 'Loss/total_loss': 1.1609173,\n"," 'learning_rate': 0.07995972}\n","INFO:tensorflow:Step 1800 per-step time 0.165s\n","I1127 01:30:59.398243 135971017113600 model_lib_v2.py:705] Step 1800 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.52339107,\n"," 'Loss/localization_loss': 0.4896537,\n"," 'Loss/regularization_loss': 0.15989858,\n"," 'Loss/total_loss': 1.1729434,\n"," 'learning_rate': 0.0799474}\n","I1127 01:30:59.398582 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.52339107,\n"," 'Loss/localization_loss': 0.4896537,\n"," 'Loss/regularization_loss': 0.15989858,\n"," 'Loss/total_loss': 1.1729434,\n"," 'learning_rate': 0.0799474}\n","INFO:tensorflow:Step 1900 per-step time 0.167s\n","I1127 01:31:16.060345 135971017113600 model_lib_v2.py:705] Step 1900 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6522007,\n"," 'Loss/localization_loss': 0.42320958,\n"," 'Loss/regularization_loss': 0.1603472,\n"," 'Loss/total_loss': 1.2357575,\n"," 'learning_rate': 0.07993342}\n","I1127 01:31:16.060698 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.6522007,\n"," 'Loss/localization_loss': 0.42320958,\n"," 'Loss/regularization_loss': 0.1603472,\n"," 'Loss/total_loss': 1.2357575,\n"," 'learning_rate': 0.07993342}\n","INFO:tensorflow:Step 2000 per-step time 0.168s\n","I1127 01:31:32.825207 135971017113600 model_lib_v2.py:705] Step 2000 per-step time 0.168s\n","INFO:tensorflow:{'Loss/classification_loss': 0.48635677,\n"," 'Loss/localization_loss': 0.45228174,\n"," 'Loss/regularization_loss': 0.16054489,\n"," 'Loss/total_loss': 1.0991834,\n"," 'learning_rate': 0.07991781}\n","I1127 01:31:32.825559 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.48635677,\n"," 'Loss/localization_loss': 0.45228174,\n"," 'Loss/regularization_loss': 0.16054489,\n"," 'Loss/total_loss': 1.0991834,\n"," 'learning_rate': 0.07991781}\n","INFO:tensorflow:Step 2100 per-step time 0.177s\n","I1127 01:31:50.563667 135971017113600 model_lib_v2.py:705] Step 2100 per-step time 0.177s\n","INFO:tensorflow:{'Loss/classification_loss': 0.48535866,\n"," 'Loss/localization_loss': 0.46248123,\n"," 'Loss/regularization_loss': 0.16073902,\n"," 'Loss/total_loss': 1.1085789,\n"," 'learning_rate': 0.07990056}\n","I1127 01:31:50.563990 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.48535866,\n"," 'Loss/localization_loss': 0.46248123,\n"," 'Loss/regularization_loss': 0.16073902,\n"," 'Loss/total_loss': 1.1085789,\n"," 'learning_rate': 0.07990056}\n","INFO:tensorflow:Step 2200 per-step time 0.170s\n","I1127 01:32:07.597116 135971017113600 model_lib_v2.py:705] Step 2200 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.465314,\n"," 'Loss/localization_loss': 0.4417945,\n"," 'Loss/regularization_loss': 0.16127047,\n"," 'Loss/total_loss': 1.068379,\n"," 'learning_rate': 0.07988167}\n","I1127 01:32:07.597468 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.465314,\n"," 'Loss/localization_loss': 0.4417945,\n"," 'Loss/regularization_loss': 0.16127047,\n"," 'Loss/total_loss': 1.068379,\n"," 'learning_rate': 0.07988167}\n","INFO:tensorflow:Step 2300 per-step time 0.165s\n","I1127 01:32:24.120540 135971017113600 model_lib_v2.py:705] Step 2300 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.45290232,\n"," 'Loss/localization_loss': 0.3448508,\n"," 'Loss/regularization_loss': 0.16173781,\n"," 'Loss/total_loss': 0.9594909,\n"," 'learning_rate': 0.07986114}\n","I1127 01:32:24.121035 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.45290232,\n"," 'Loss/localization_loss': 0.3448508,\n"," 'Loss/regularization_loss': 0.16173781,\n"," 'Loss/total_loss': 0.9594909,\n"," 'learning_rate': 0.07986114}\n","INFO:tensorflow:Step 2400 per-step time 0.167s\n","I1127 01:32:40.793360 135971017113600 model_lib_v2.py:705] Step 2400 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.3939681,\n"," 'Loss/localization_loss': 0.36017853,\n"," 'Loss/regularization_loss': 0.16195467,\n"," 'Loss/total_loss': 0.91610134,\n"," 'learning_rate': 0.07983897}\n","I1127 01:32:40.793777 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.3939681,\n"," 'Loss/localization_loss': 0.36017853,\n"," 'Loss/regularization_loss': 0.16195467,\n"," 'Loss/total_loss': 0.91610134,\n"," 'learning_rate': 0.07983897}\n","INFO:tensorflow:Step 2500 per-step time 0.173s\n","I1127 01:32:58.108922 135971017113600 model_lib_v2.py:705] Step 2500 per-step time 0.173s\n","INFO:tensorflow:{'Loss/classification_loss': 0.5436363,\n"," 'Loss/localization_loss': 0.39591885,\n"," 'Loss/regularization_loss': 0.16237308,\n"," 'Loss/total_loss': 1.1019282,\n"," 'learning_rate': 0.079815164}\n","I1127 01:32:58.109285 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.5436363,\n"," 'Loss/localization_loss': 0.39591885,\n"," 'Loss/regularization_loss': 0.16237308,\n"," 'Loss/total_loss': 1.1019282,\n"," 'learning_rate': 0.079815164}\n","INFO:tensorflow:Step 2600 per-step time 0.165s\n","I1127 01:33:14.626250 135971017113600 model_lib_v2.py:705] Step 2600 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.49092215,\n"," 'Loss/localization_loss': 0.438832,\n"," 'Loss/regularization_loss': 0.16282406,\n"," 'Loss/total_loss': 1.0925782,\n"," 'learning_rate': 0.07978972}\n","I1127 01:33:14.626585 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.49092215,\n"," 'Loss/localization_loss': 0.438832,\n"," 'Loss/regularization_loss': 0.16282406,\n"," 'Loss/total_loss': 1.0925782,\n"," 'learning_rate': 0.07978972}\n","INFO:tensorflow:Step 2700 per-step time 0.165s\n","I1127 01:33:31.144127 135971017113600 model_lib_v2.py:705] Step 2700 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.54826117,\n"," 'Loss/localization_loss': 0.41100878,\n"," 'Loss/regularization_loss': 0.1632202,\n"," 'Loss/total_loss': 1.1224902,\n"," 'learning_rate': 0.07976264}\n","I1127 01:33:31.144422 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.54826117,\n"," 'Loss/localization_loss': 0.41100878,\n"," 'Loss/regularization_loss': 0.1632202,\n"," 'Loss/total_loss': 1.1224902,\n"," 'learning_rate': 0.07976264}\n","INFO:tensorflow:Step 2800 per-step time 0.167s\n","I1127 01:33:47.879856 135971017113600 model_lib_v2.py:705] Step 2800 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.50204206,\n"," 'Loss/localization_loss': 0.34750012,\n"," 'Loss/regularization_loss': 0.16346,\n"," 'Loss/total_loss': 1.0130022,\n"," 'learning_rate': 0.07973392}\n","I1127 01:33:47.880152 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.50204206,\n"," 'Loss/localization_loss': 0.34750012,\n"," 'Loss/regularization_loss': 0.16346,\n"," 'Loss/total_loss': 1.0130022,\n"," 'learning_rate': 0.07973392}\n","INFO:tensorflow:Step 2900 per-step time 0.166s\n","I1127 01:34:04.514831 135971017113600 model_lib_v2.py:705] Step 2900 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.6314599,\n"," 'Loss/localization_loss': 0.59657776,\n"," 'Loss/regularization_loss': 0.16397597,\n"," 'Loss/total_loss': 1.3920135,\n"," 'learning_rate': 0.07970358}\n","I1127 01:34:04.515145 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.6314599,\n"," 'Loss/localization_loss': 0.59657776,\n"," 'Loss/regularization_loss': 0.16397597,\n"," 'Loss/total_loss': 1.3920135,\n"," 'learning_rate': 0.07970358}\n","INFO:tensorflow:Step 3000 per-step time 0.166s\n","I1127 01:34:21.084625 135971017113600 model_lib_v2.py:705] Step 3000 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.38952205,\n"," 'Loss/localization_loss': 0.318429,\n"," 'Loss/regularization_loss': 0.16431484,\n"," 'Loss/total_loss': 0.87226593,\n"," 'learning_rate': 0.0796716}\n","I1127 01:34:21.084941 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.38952205,\n"," 'Loss/localization_loss': 0.318429,\n"," 'Loss/regularization_loss': 0.16431484,\n"," 'Loss/total_loss': 0.87226593,\n"," 'learning_rate': 0.0796716}\n","INFO:tensorflow:Step 3100 per-step time 0.170s\n","I1127 01:34:38.112239 135971017113600 model_lib_v2.py:705] Step 3100 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.5761385,\n"," 'Loss/localization_loss': 0.5554373,\n"," 'Loss/regularization_loss': 0.16462848,\n"," 'Loss/total_loss': 1.2962043,\n"," 'learning_rate': 0.07963799}\n","I1127 01:34:38.112582 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.5761385,\n"," 'Loss/localization_loss': 0.5554373,\n"," 'Loss/regularization_loss': 0.16462848,\n"," 'Loss/total_loss': 1.2962043,\n"," 'learning_rate': 0.07963799}\n","INFO:tensorflow:Step 3200 per-step time 0.164s\n","I1127 01:34:54.538326 135971017113600 model_lib_v2.py:705] Step 3200 per-step time 0.164s\n","INFO:tensorflow:{'Loss/classification_loss': 0.42717916,\n"," 'Loss/localization_loss': 0.31981277,\n"," 'Loss/regularization_loss': 0.16505595,\n"," 'Loss/total_loss': 0.91204786,\n"," 'learning_rate': 0.07960275}\n","I1127 01:34:54.538714 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.42717916,\n"," 'Loss/localization_loss': 0.31981277,\n"," 'Loss/regularization_loss': 0.16505595,\n"," 'Loss/total_loss': 0.91204786,\n"," 'learning_rate': 0.07960275}\n","INFO:tensorflow:Step 3300 per-step time 0.166s\n","I1127 01:35:11.106337 135971017113600 model_lib_v2.py:705] Step 3300 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.38042867,\n"," 'Loss/localization_loss': 0.27199927,\n"," 'Loss/regularization_loss': 0.16523646,\n"," 'Loss/total_loss': 0.8176644,\n"," 'learning_rate': 0.07956588}\n","I1127 01:35:11.106649 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.38042867,\n"," 'Loss/localization_loss': 0.27199927,\n"," 'Loss/regularization_loss': 0.16523646,\n"," 'Loss/total_loss': 0.8176644,\n"," 'learning_rate': 0.07956588}\n","INFO:tensorflow:Step 3400 per-step time 0.164s\n","I1127 01:35:27.536094 135971017113600 model_lib_v2.py:705] Step 3400 per-step time 0.164s\n","INFO:tensorflow:{'Loss/classification_loss': 0.46944568,\n"," 'Loss/localization_loss': 0.35571638,\n"," 'Loss/regularization_loss': 0.16575584,\n"," 'Loss/total_loss': 0.9909179,\n"," 'learning_rate': 0.079527386}\n","I1127 01:35:27.536429 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.46944568,\n"," 'Loss/localization_loss': 0.35571638,\n"," 'Loss/regularization_loss': 0.16575584,\n"," 'Loss/total_loss': 0.9909179,\n"," 'learning_rate': 0.079527386}\n","INFO:tensorflow:Step 3500 per-step time 0.171s\n","I1127 01:35:44.650537 135971017113600 model_lib_v2.py:705] Step 3500 per-step time 0.171s\n","INFO:tensorflow:{'Loss/classification_loss': 0.42094013,\n"," 'Loss/localization_loss': 0.3232546,\n"," 'Loss/regularization_loss': 0.16617727,\n"," 'Loss/total_loss': 0.910372,\n"," 'learning_rate': 0.07948727}\n","I1127 01:35:44.650860 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.42094013,\n"," 'Loss/localization_loss': 0.3232546,\n"," 'Loss/regularization_loss': 0.16617727,\n"," 'Loss/total_loss': 0.910372,\n"," 'learning_rate': 0.07948727}\n","INFO:tensorflow:Step 3600 per-step time 0.164s\n","I1127 01:36:01.062427 135971017113600 model_lib_v2.py:705] Step 3600 per-step time 0.164s\n","INFO:tensorflow:{'Loss/classification_loss': 0.64008164,\n"," 'Loss/localization_loss': 0.41341653,\n"," 'Loss/regularization_loss': 0.16631508,\n"," 'Loss/total_loss': 1.2198132,\n"," 'learning_rate': 0.079445526}\n","I1127 01:36:01.062749 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.64008164,\n"," 'Loss/localization_loss': 0.41341653,\n"," 'Loss/regularization_loss': 0.16631508,\n"," 'Loss/total_loss': 1.2198132,\n"," 'learning_rate': 0.079445526}\n","INFO:tensorflow:Step 3700 per-step time 0.166s\n","I1127 01:36:17.691009 135971017113600 model_lib_v2.py:705] Step 3700 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.3609404,\n"," 'Loss/localization_loss': 0.29416063,\n"," 'Loss/regularization_loss': 0.16667576,\n"," 'Loss/total_loss': 0.8217768,\n"," 'learning_rate': 0.07940216}\n","I1127 01:36:17.691350 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.3609404,\n"," 'Loss/localization_loss': 0.29416063,\n"," 'Loss/regularization_loss': 0.16667576,\n"," 'Loss/total_loss': 0.8217768,\n"," 'learning_rate': 0.07940216}\n","INFO:tensorflow:Step 3800 per-step time 0.169s\n","I1127 01:36:34.558619 135971017113600 model_lib_v2.py:705] Step 3800 per-step time 0.169s\n","INFO:tensorflow:{'Loss/classification_loss': 0.36869705,\n"," 'Loss/localization_loss': 0.319586,\n"," 'Loss/regularization_loss': 0.16697158,\n"," 'Loss/total_loss': 0.85525465,\n"," 'learning_rate': 0.079357184}\n","I1127 01:36:34.558960 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.36869705,\n"," 'Loss/localization_loss': 0.319586,\n"," 'Loss/regularization_loss': 0.16697158,\n"," 'Loss/total_loss': 0.85525465,\n"," 'learning_rate': 0.079357184}\n","INFO:tensorflow:Step 3900 per-step time 0.170s\n","I1127 01:36:51.556985 135971017113600 model_lib_v2.py:705] Step 3900 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.387497,\n"," 'Loss/localization_loss': 0.26923805,\n"," 'Loss/regularization_loss': 0.16737281,\n"," 'Loss/total_loss': 0.8241079,\n"," 'learning_rate': 0.07931058}\n","I1127 01:36:51.557341 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.387497,\n"," 'Loss/localization_loss': 0.26923805,\n"," 'Loss/regularization_loss': 0.16737281,\n"," 'Loss/total_loss': 0.8241079,\n"," 'learning_rate': 0.07931058}\n","INFO:tensorflow:Step 4000 per-step time 0.169s\n","I1127 01:37:08.441771 135971017113600 model_lib_v2.py:705] Step 4000 per-step time 0.169s\n","INFO:tensorflow:{'Loss/classification_loss': 0.52233255,\n"," 'Loss/localization_loss': 0.36532974,\n"," 'Loss/regularization_loss': 0.16773973,\n"," 'Loss/total_loss': 1.055402,\n"," 'learning_rate': 0.07926236}\n","I1127 01:37:08.442106 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.52233255,\n"," 'Loss/localization_loss': 0.36532974,\n"," 'Loss/regularization_loss': 0.16773973,\n"," 'Loss/total_loss': 1.055402,\n"," 'learning_rate': 0.07926236}\n","INFO:tensorflow:Step 4100 per-step time 0.170s\n","I1127 01:37:25.479699 135971017113600 model_lib_v2.py:705] Step 4100 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.38450292,\n"," 'Loss/localization_loss': 0.31211787,\n"," 'Loss/regularization_loss': 0.1681437,\n"," 'Loss/total_loss': 0.8647645,\n"," 'learning_rate': 0.07921253}\n","I1127 01:37:25.480018 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.38450292,\n"," 'Loss/localization_loss': 0.31211787,\n"," 'Loss/regularization_loss': 0.1681437,\n"," 'Loss/total_loss': 0.8647645,\n"," 'learning_rate': 0.07921253}\n","INFO:tensorflow:Step 4200 per-step time 0.165s\n","I1127 01:37:41.944560 135971017113600 model_lib_v2.py:705] Step 4200 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.41472405,\n"," 'Loss/localization_loss': 0.36627898,\n"," 'Loss/regularization_loss': 0.16849433,\n"," 'Loss/total_loss': 0.94949734,\n"," 'learning_rate': 0.07916109}\n","I1127 01:37:41.944888 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.41472405,\n"," 'Loss/localization_loss': 0.36627898,\n"," 'Loss/regularization_loss': 0.16849433,\n"," 'Loss/total_loss': 0.94949734,\n"," 'learning_rate': 0.07916109}\n","INFO:tensorflow:Step 4300 per-step time 0.167s\n","I1127 01:37:58.645138 135971017113600 model_lib_v2.py:705] Step 4300 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.36275813,\n"," 'Loss/localization_loss': 0.27623677,\n"," 'Loss/regularization_loss': 0.1687756,\n"," 'Loss/total_loss': 0.80777055,\n"," 'learning_rate': 0.07910804}\n","I1127 01:37:58.645546 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.36275813,\n"," 'Loss/localization_loss': 0.27623677,\n"," 'Loss/regularization_loss': 0.1687756,\n"," 'Loss/total_loss': 0.80777055,\n"," 'learning_rate': 0.07910804}\n","INFO:tensorflow:Step 4400 per-step time 0.163s\n","I1127 01:38:14.982877 135971017113600 model_lib_v2.py:705] Step 4400 per-step time 0.163s\n","INFO:tensorflow:{'Loss/classification_loss': 0.4114683,\n"," 'Loss/localization_loss': 0.38711885,\n"," 'Loss/regularization_loss': 0.16919051,\n"," 'Loss/total_loss': 0.96777767,\n"," 'learning_rate': 0.07905338}\n","I1127 01:38:14.983174 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.4114683,\n"," 'Loss/localization_loss': 0.38711885,\n"," 'Loss/regularization_loss': 0.16919051,\n"," 'Loss/total_loss': 0.96777767,\n"," 'learning_rate': 0.07905338}\n","INFO:tensorflow:Step 4500 per-step time 0.167s\n","I1127 01:38:31.637721 135971017113600 model_lib_v2.py:705] Step 4500 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.33656165,\n"," 'Loss/localization_loss': 0.19499983,\n"," 'Loss/regularization_loss': 0.16961162,\n"," 'Loss/total_loss': 0.7011731,\n"," 'learning_rate': 0.07899711}\n","I1127 01:38:31.638045 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.33656165,\n"," 'Loss/localization_loss': 0.19499983,\n"," 'Loss/regularization_loss': 0.16961162,\n"," 'Loss/total_loss': 0.7011731,\n"," 'learning_rate': 0.07899711}\n","INFO:tensorflow:Step 4600 per-step time 0.160s\n","I1127 01:38:47.671144 135971017113600 model_lib_v2.py:705] Step 4600 per-step time 0.160s\n","INFO:tensorflow:{'Loss/classification_loss': 0.43051505,\n"," 'Loss/localization_loss': 0.4176933,\n"," 'Loss/regularization_loss': 0.17011246,\n"," 'Loss/total_loss': 1.0183208,\n"," 'learning_rate': 0.078939244}\n","I1127 01:38:47.671433 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.43051505,\n"," 'Loss/localization_loss': 0.4176933,\n"," 'Loss/regularization_loss': 0.17011246,\n"," 'Loss/total_loss': 1.0183208,\n"," 'learning_rate': 0.078939244}\n","INFO:tensorflow:Step 4700 per-step time 0.161s\n","I1127 01:39:03.793059 135971017113600 model_lib_v2.py:705] Step 4700 per-step time 0.161s\n","INFO:tensorflow:{'Loss/classification_loss': 0.45189545,\n"," 'Loss/localization_loss': 0.40681255,\n"," 'Loss/regularization_loss': 0.17044404,\n"," 'Loss/total_loss': 1.029152,\n"," 'learning_rate': 0.07887978}\n","I1127 01:39:03.793371 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.45189545,\n"," 'Loss/localization_loss': 0.40681255,\n"," 'Loss/regularization_loss': 0.17044404,\n"," 'Loss/total_loss': 1.029152,\n"," 'learning_rate': 0.07887978}\n","INFO:tensorflow:Step 4800 per-step time 0.165s\n","I1127 01:39:20.285744 135971017113600 model_lib_v2.py:705] Step 4800 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.42739308,\n"," 'Loss/localization_loss': 0.4169832,\n"," 'Loss/regularization_loss': 0.17067543,\n"," 'Loss/total_loss': 1.0150517,\n"," 'learning_rate': 0.07881871}\n","I1127 01:39:20.286101 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.42739308,\n"," 'Loss/localization_loss': 0.4169832,\n"," 'Loss/regularization_loss': 0.17067543,\n"," 'Loss/total_loss': 1.0150517,\n"," 'learning_rate': 0.07881871}\n","INFO:tensorflow:Step 4900 per-step time 0.172s\n","I1127 01:39:37.449208 135971017113600 model_lib_v2.py:705] Step 4900 per-step time 0.172s\n","INFO:tensorflow:{'Loss/classification_loss': 0.40219352,\n"," 'Loss/localization_loss': 0.33136502,\n"," 'Loss/regularization_loss': 0.17096078,\n"," 'Loss/total_loss': 0.9045193,\n"," 'learning_rate': 0.07875605}\n","I1127 01:39:37.449607 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.40219352,\n"," 'Loss/localization_loss': 0.33136502,\n"," 'Loss/regularization_loss': 0.17096078,\n"," 'Loss/total_loss': 0.9045193,\n"," 'learning_rate': 0.07875605}\n","INFO:tensorflow:Step 5000 per-step time 0.166s\n","I1127 01:39:54.015362 135971017113600 model_lib_v2.py:705] Step 5000 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.4012538,\n"," 'Loss/localization_loss': 0.3560111,\n"," 'Loss/regularization_loss': 0.17117275,\n"," 'Loss/total_loss': 0.9284376,\n"," 'learning_rate': 0.078691795}\n","I1127 01:39:54.015735 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.4012538,\n"," 'Loss/localization_loss': 0.3560111,\n"," 'Loss/regularization_loss': 0.17117275,\n"," 'Loss/total_loss': 0.9284376,\n"," 'learning_rate': 0.078691795}\n","INFO:tensorflow:Step 5100 per-step time 0.174s\n","I1127 01:40:11.373936 135971017113600 model_lib_v2.py:705] Step 5100 per-step time 0.174s\n","INFO:tensorflow:{'Loss/classification_loss': 0.36447394,\n"," 'Loss/localization_loss': 0.30265734,\n"," 'Loss/regularization_loss': 0.17153217,\n"," 'Loss/total_loss': 0.83866346,\n"," 'learning_rate': 0.07862595}\n","I1127 01:40:11.374376 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.36447394,\n"," 'Loss/localization_loss': 0.30265734,\n"," 'Loss/regularization_loss': 0.17153217,\n"," 'Loss/total_loss': 0.83866346,\n"," 'learning_rate': 0.07862595}\n","INFO:tensorflow:Step 5200 per-step time 0.169s\n","I1127 01:40:28.285423 135971017113600 model_lib_v2.py:705] Step 5200 per-step time 0.169s\n","INFO:tensorflow:{'Loss/classification_loss': 0.31090847,\n"," 'Loss/localization_loss': 0.2411159,\n"," 'Loss/regularization_loss': 0.17162324,\n"," 'Loss/total_loss': 0.7236476,\n"," 'learning_rate': 0.07855851}\n","I1127 01:40:28.285821 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.31090847,\n"," 'Loss/localization_loss': 0.2411159,\n"," 'Loss/regularization_loss': 0.17162324,\n"," 'Loss/total_loss': 0.7236476,\n"," 'learning_rate': 0.07855851}\n","INFO:tensorflow:Step 5300 per-step time 0.171s\n","I1127 01:40:45.360335 135971017113600 model_lib_v2.py:705] Step 5300 per-step time 0.171s\n","INFO:tensorflow:{'Loss/classification_loss': 0.36500657,\n"," 'Loss/localization_loss': 0.33528635,\n"," 'Loss/regularization_loss': 0.17231967,\n"," 'Loss/total_loss': 0.8726126,\n"," 'learning_rate': 0.07848949}\n","I1127 01:40:45.360666 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.36500657,\n"," 'Loss/localization_loss': 0.33528635,\n"," 'Loss/regularization_loss': 0.17231967,\n"," 'Loss/total_loss': 0.8726126,\n"," 'learning_rate': 0.07848949}\n","INFO:tensorflow:Step 5400 per-step time 0.162s\n","I1127 01:41:01.534182 135971017113600 model_lib_v2.py:705] Step 5400 per-step time 0.162s\n","INFO:tensorflow:{'Loss/classification_loss': 0.31536096,\n"," 'Loss/localization_loss': 0.2627114,\n"," 'Loss/regularization_loss': 0.17251435,\n"," 'Loss/total_loss': 0.75058675,\n"," 'learning_rate': 0.078418896}\n","I1127 01:41:01.534537 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.31536096,\n"," 'Loss/localization_loss': 0.2627114,\n"," 'Loss/regularization_loss': 0.17251435,\n"," 'Loss/total_loss': 0.75058675,\n"," 'learning_rate': 0.078418896}\n","INFO:tensorflow:Step 5500 per-step time 0.161s\n","I1127 01:41:17.594150 135971017113600 model_lib_v2.py:705] Step 5500 per-step time 0.161s\n","INFO:tensorflow:{'Loss/classification_loss': 0.26295525,\n"," 'Loss/localization_loss': 0.24438365,\n"," 'Loss/regularization_loss': 0.17296807,\n"," 'Loss/total_loss': 0.680307,\n"," 'learning_rate': 0.078346714}\n","I1127 01:41:17.594446 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.26295525,\n"," 'Loss/localization_loss': 0.24438365,\n"," 'Loss/regularization_loss': 0.17296807,\n"," 'Loss/total_loss': 0.680307,\n"," 'learning_rate': 0.078346714}\n","INFO:tensorflow:Step 5600 per-step time 0.165s\n","I1127 01:41:34.104209 135971017113600 model_lib_v2.py:705] Step 5600 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.25896886,\n"," 'Loss/localization_loss': 0.21595256,\n"," 'Loss/regularization_loss': 0.17326838,\n"," 'Loss/total_loss': 0.6481898,\n"," 'learning_rate': 0.07827295}\n","I1127 01:41:34.104551 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.25896886,\n"," 'Loss/localization_loss': 0.21595256,\n"," 'Loss/regularization_loss': 0.17326838,\n"," 'Loss/total_loss': 0.6481898,\n"," 'learning_rate': 0.07827295}\n","INFO:tensorflow:Step 5700 per-step time 0.165s\n","I1127 01:41:50.567962 135971017113600 model_lib_v2.py:705] Step 5700 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.32821998,\n"," 'Loss/localization_loss': 0.35324985,\n"," 'Loss/regularization_loss': 0.17362732,\n"," 'Loss/total_loss': 0.8550971,\n"," 'learning_rate': 0.07819763}\n","I1127 01:41:50.568271 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.32821998,\n"," 'Loss/localization_loss': 0.35324985,\n"," 'Loss/regularization_loss': 0.17362732,\n"," 'Loss/total_loss': 0.8550971,\n"," 'learning_rate': 0.07819763}\n","INFO:tensorflow:Step 5800 per-step time 0.166s\n","I1127 01:42:07.124417 135971017113600 model_lib_v2.py:705] Step 5800 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.29928973,\n"," 'Loss/localization_loss': 0.21732491,\n"," 'Loss/regularization_loss': 0.17393191,\n"," 'Loss/total_loss': 0.6905466,\n"," 'learning_rate': 0.07812072}\n","I1127 01:42:07.124755 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.29928973,\n"," 'Loss/localization_loss': 0.21732491,\n"," 'Loss/regularization_loss': 0.17393191,\n"," 'Loss/total_loss': 0.6905466,\n"," 'learning_rate': 0.07812072}\n","INFO:tensorflow:Step 5900 per-step time 0.166s\n","I1127 01:42:23.696503 135971017113600 model_lib_v2.py:705] Step 5900 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.36386153,\n"," 'Loss/localization_loss': 0.20457149,\n"," 'Loss/regularization_loss': 0.17412113,\n"," 'Loss/total_loss': 0.7425542,\n"," 'learning_rate': 0.078042254}\n","I1127 01:42:23.696802 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.36386153,\n"," 'Loss/localization_loss': 0.20457149,\n"," 'Loss/regularization_loss': 0.17412113,\n"," 'Loss/total_loss': 0.7425542,\n"," 'learning_rate': 0.078042254}\n","INFO:tensorflow:Step 6000 per-step time 0.164s\n","I1127 01:42:40.131939 135971017113600 model_lib_v2.py:705] Step 6000 per-step time 0.164s\n","INFO:tensorflow:{'Loss/classification_loss': 0.33560678,\n"," 'Loss/localization_loss': 0.28238815,\n"," 'Loss/regularization_loss': 0.17424671,\n"," 'Loss/total_loss': 0.79224163,\n"," 'learning_rate': 0.07796223}\n","I1127 01:42:40.132263 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.33560678,\n"," 'Loss/localization_loss': 0.28238815,\n"," 'Loss/regularization_loss': 0.17424671,\n"," 'Loss/total_loss': 0.79224163,\n"," 'learning_rate': 0.07796223}\n","INFO:tensorflow:Step 6100 per-step time 0.170s\n","I1127 01:42:57.140271 135971017113600 model_lib_v2.py:705] Step 6100 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.3757382,\n"," 'Loss/localization_loss': 0.40729162,\n"," 'Loss/regularization_loss': 0.1745351,\n"," 'Loss/total_loss': 0.9575649,\n"," 'learning_rate': 0.077880636}\n","I1127 01:42:57.140642 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.3757382,\n"," 'Loss/localization_loss': 0.40729162,\n"," 'Loss/regularization_loss': 0.1745351,\n"," 'Loss/total_loss': 0.9575649,\n"," 'learning_rate': 0.077880636}\n","INFO:tensorflow:Step 6200 per-step time 0.168s\n","I1127 01:43:13.904399 135971017113600 model_lib_v2.py:705] Step 6200 per-step time 0.168s\n","INFO:tensorflow:{'Loss/classification_loss': 0.32353926,\n"," 'Loss/localization_loss': 0.21298495,\n"," 'Loss/regularization_loss': 0.17482917,\n"," 'Loss/total_loss': 0.71135336,\n"," 'learning_rate': 0.07779749}\n","I1127 01:43:13.904766 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.32353926,\n"," 'Loss/localization_loss': 0.21298495,\n"," 'Loss/regularization_loss': 0.17482917,\n"," 'Loss/total_loss': 0.71135336,\n"," 'learning_rate': 0.07779749}\n","INFO:tensorflow:Step 6300 per-step time 0.169s\n","I1127 01:43:30.802856 135971017113600 model_lib_v2.py:705] Step 6300 per-step time 0.169s\n","INFO:tensorflow:{'Loss/classification_loss': 0.36770758,\n"," 'Loss/localization_loss': 0.27008876,\n"," 'Loss/regularization_loss': 0.17489085,\n"," 'Loss/total_loss': 0.81268716,\n"," 'learning_rate': 0.07771279}\n","I1127 01:43:30.803191 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.36770758,\n"," 'Loss/localization_loss': 0.27008876,\n"," 'Loss/regularization_loss': 0.17489085,\n"," 'Loss/total_loss': 0.81268716,\n"," 'learning_rate': 0.07771279}\n","INFO:tensorflow:Step 6400 per-step time 0.171s\n","I1127 01:43:47.895938 135971017113600 model_lib_v2.py:705] Step 6400 per-step time 0.171s\n","INFO:tensorflow:{'Loss/classification_loss': 0.29755703,\n"," 'Loss/localization_loss': 0.32312852,\n"," 'Loss/regularization_loss': 0.17520587,\n"," 'Loss/total_loss': 0.79589146,\n"," 'learning_rate': 0.077626534}\n","I1127 01:43:47.896313 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.29755703,\n"," 'Loss/localization_loss': 0.32312852,\n"," 'Loss/regularization_loss': 0.17520587,\n"," 'Loss/total_loss': 0.79589146,\n"," 'learning_rate': 0.077626534}\n","INFO:tensorflow:Step 6500 per-step time 0.168s\n","I1127 01:44:04.675580 135971017113600 model_lib_v2.py:705] Step 6500 per-step time 0.168s\n","INFO:tensorflow:{'Loss/classification_loss': 0.37529874,\n"," 'Loss/localization_loss': 0.28749862,\n"," 'Loss/regularization_loss': 0.17536154,\n"," 'Loss/total_loss': 0.83815885,\n"," 'learning_rate': 0.077538736}\n","I1127 01:44:04.675932 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.37529874,\n"," 'Loss/localization_loss': 0.28749862,\n"," 'Loss/regularization_loss': 0.17536154,\n"," 'Loss/total_loss': 0.83815885,\n"," 'learning_rate': 0.077538736}\n","INFO:tensorflow:Step 6600 per-step time 0.167s\n","I1127 01:44:21.350861 135971017113600 model_lib_v2.py:705] Step 6600 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.25252512,\n"," 'Loss/localization_loss': 0.2410812,\n"," 'Loss/regularization_loss': 0.17582095,\n"," 'Loss/total_loss': 0.6694273,\n"," 'learning_rate': 0.077449396}\n","I1127 01:44:21.351200 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.25252512,\n"," 'Loss/localization_loss': 0.2410812,\n"," 'Loss/regularization_loss': 0.17582095,\n"," 'Loss/total_loss': 0.6694273,\n"," 'learning_rate': 0.077449396}\n","INFO:tensorflow:Step 6700 per-step time 0.170s\n","I1127 01:44:38.306612 135971017113600 model_lib_v2.py:705] Step 6700 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2713687,\n"," 'Loss/localization_loss': 0.22997348,\n"," 'Loss/regularization_loss': 0.17605594,\n"," 'Loss/total_loss': 0.6773981,\n"," 'learning_rate': 0.077358514}\n","I1127 01:44:38.306919 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.2713687,\n"," 'Loss/localization_loss': 0.22997348,\n"," 'Loss/regularization_loss': 0.17605594,\n"," 'Loss/total_loss': 0.6773981,\n"," 'learning_rate': 0.077358514}\n","INFO:tensorflow:Step 6800 per-step time 0.167s\n","I1127 01:44:55.007755 135971017113600 model_lib_v2.py:705] Step 6800 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.3081409,\n"," 'Loss/localization_loss': 0.22705863,\n"," 'Loss/regularization_loss': 0.17625682,\n"," 'Loss/total_loss': 0.71145636,\n"," 'learning_rate': 0.0772661}\n","I1127 01:44:55.008059 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.3081409,\n"," 'Loss/localization_loss': 0.22705863,\n"," 'Loss/regularization_loss': 0.17625682,\n"," 'Loss/total_loss': 0.71145636,\n"," 'learning_rate': 0.0772661}\n","INFO:tensorflow:Step 6900 per-step time 0.166s\n","I1127 01:45:11.579389 135971017113600 model_lib_v2.py:705] Step 6900 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.28277016,\n"," 'Loss/localization_loss': 0.27779675,\n"," 'Loss/regularization_loss': 0.17662878,\n"," 'Loss/total_loss': 0.7371957,\n"," 'learning_rate': 0.077172145}\n","I1127 01:45:11.579721 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.28277016,\n"," 'Loss/localization_loss': 0.27779675,\n"," 'Loss/regularization_loss': 0.17662878,\n"," 'Loss/total_loss': 0.7371957,\n"," 'learning_rate': 0.077172145}\n","INFO:tensorflow:Step 7000 per-step time 0.165s\n","I1127 01:45:28.118482 135971017113600 model_lib_v2.py:705] Step 7000 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.32065737,\n"," 'Loss/localization_loss': 0.27176705,\n"," 'Loss/regularization_loss': 0.17714639,\n"," 'Loss/total_loss': 0.76957077,\n"," 'learning_rate': 0.07707667}\n","I1127 01:45:28.118778 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.32065737,\n"," 'Loss/localization_loss': 0.27176705,\n"," 'Loss/regularization_loss': 0.17714639,\n"," 'Loss/total_loss': 0.76957077,\n"," 'learning_rate': 0.07707667}\n","INFO:tensorflow:Step 7100 per-step time 0.171s\n","I1127 01:45:45.170292 135971017113600 model_lib_v2.py:705] Step 7100 per-step time 0.171s\n","INFO:tensorflow:{'Loss/classification_loss': 0.30813298,\n"," 'Loss/localization_loss': 0.15929838,\n"," 'Loss/regularization_loss': 0.17758863,\n"," 'Loss/total_loss': 0.64502,\n"," 'learning_rate': 0.07697967}\n","I1127 01:45:45.170614 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.30813298,\n"," 'Loss/localization_loss': 0.15929838,\n"," 'Loss/regularization_loss': 0.17758863,\n"," 'Loss/total_loss': 0.64502,\n"," 'learning_rate': 0.07697967}\n","INFO:tensorflow:Step 7200 per-step time 0.164s\n","I1127 01:46:01.548440 135971017113600 model_lib_v2.py:705] Step 7200 per-step time 0.164s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2705037,\n"," 'Loss/localization_loss': 0.29314303,\n"," 'Loss/regularization_loss': 0.17802334,\n"," 'Loss/total_loss': 0.7416701,\n"," 'learning_rate': 0.07688115}\n","I1127 01:46:01.548774 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.2705037,\n"," 'Loss/localization_loss': 0.29314303,\n"," 'Loss/regularization_loss': 0.17802334,\n"," 'Loss/total_loss': 0.7416701,\n"," 'learning_rate': 0.07688115}\n","INFO:tensorflow:Step 7300 per-step time 0.167s\n","I1127 01:46:18.208729 135971017113600 model_lib_v2.py:705] Step 7300 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2765846,\n"," 'Loss/localization_loss': 0.15572606,\n"," 'Loss/regularization_loss': 0.17835182,\n"," 'Loss/total_loss': 0.61066246,\n"," 'learning_rate': 0.07678111}\n","I1127 01:46:18.209080 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.2765846,\n"," 'Loss/localization_loss': 0.15572606,\n"," 'Loss/regularization_loss': 0.17835182,\n"," 'Loss/total_loss': 0.61066246,\n"," 'learning_rate': 0.07678111}\n","INFO:tensorflow:Step 7400 per-step time 0.166s\n","I1127 01:46:34.790816 135971017113600 model_lib_v2.py:705] Step 7400 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2468271,\n"," 'Loss/localization_loss': 0.24755494,\n"," 'Loss/regularization_loss': 0.17873015,\n"," 'Loss/total_loss': 0.67311215,\n"," 'learning_rate': 0.076679565}\n","I1127 01:46:34.791150 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.2468271,\n"," 'Loss/localization_loss': 0.24755494,\n"," 'Loss/regularization_loss': 0.17873015,\n"," 'Loss/total_loss': 0.67311215,\n"," 'learning_rate': 0.076679565}\n","INFO:tensorflow:Step 7500 per-step time 0.167s\n","I1127 01:46:51.542309 135971017113600 model_lib_v2.py:705] Step 7500 per-step time 0.167s\n","INFO:tensorflow:{'Loss/classification_loss': 0.3827348,\n"," 'Loss/localization_loss': 0.28560725,\n"," 'Loss/regularization_loss': 0.17905009,\n"," 'Loss/total_loss': 0.84739214,\n"," 'learning_rate': 0.0765765}\n","I1127 01:46:51.542680 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.3827348,\n"," 'Loss/localization_loss': 0.28560725,\n"," 'Loss/regularization_loss': 0.17905009,\n"," 'Loss/total_loss': 0.84739214,\n"," 'learning_rate': 0.0765765}\n","INFO:tensorflow:Step 7600 per-step time 0.166s\n","I1127 01:47:08.186101 135971017113600 model_lib_v2.py:705] Step 7600 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.3721017,\n"," 'Loss/localization_loss': 0.40512222,\n"," 'Loss/regularization_loss': 0.179059,\n"," 'Loss/total_loss': 0.956283,\n"," 'learning_rate': 0.07647194}\n","I1127 01:47:08.186509 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.3721017,\n"," 'Loss/localization_loss': 0.40512222,\n"," 'Loss/regularization_loss': 0.179059,\n"," 'Loss/total_loss': 0.956283,\n"," 'learning_rate': 0.07647194}\n","INFO:tensorflow:Step 7700 per-step time 0.168s\n","I1127 01:47:25.020283 135971017113600 model_lib_v2.py:705] Step 7700 per-step time 0.168s\n","INFO:tensorflow:{'Loss/classification_loss': 0.31875616,\n"," 'Loss/localization_loss': 0.23035116,\n"," 'Loss/regularization_loss': 0.17931232,\n"," 'Loss/total_loss': 0.72841966,\n"," 'learning_rate': 0.07636588}\n","I1127 01:47:25.020683 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.31875616,\n"," 'Loss/localization_loss': 0.23035116,\n"," 'Loss/regularization_loss': 0.17931232,\n"," 'Loss/total_loss': 0.72841966,\n"," 'learning_rate': 0.07636588}\n","INFO:tensorflow:Step 7800 per-step time 0.193s\n","I1127 01:47:44.363657 135971017113600 model_lib_v2.py:705] Step 7800 per-step time 0.193s\n","INFO:tensorflow:{'Loss/classification_loss': 0.31263036,\n"," 'Loss/localization_loss': 0.32616928,\n"," 'Loss/regularization_loss': 0.17957339,\n"," 'Loss/total_loss': 0.8183731,\n"," 'learning_rate': 0.07625833}\n","I1127 01:47:44.364031 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.31263036,\n"," 'Loss/localization_loss': 0.32616928,\n"," 'Loss/regularization_loss': 0.17957339,\n"," 'Loss/total_loss': 0.8183731,\n"," 'learning_rate': 0.07625833}\n","INFO:tensorflow:Step 7900 per-step time 0.177s\n","I1127 01:48:02.042658 135971017113600 model_lib_v2.py:705] Step 7900 per-step time 0.177s\n","INFO:tensorflow:{'Loss/classification_loss': 0.251428,\n"," 'Loss/localization_loss': 0.23266093,\n"," 'Loss/regularization_loss': 0.17971855,\n"," 'Loss/total_loss': 0.6638075,\n"," 'learning_rate': 0.07614928}\n","I1127 01:48:02.043015 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.251428,\n"," 'Loss/localization_loss': 0.23266093,\n"," 'Loss/regularization_loss': 0.17971855,\n"," 'Loss/total_loss': 0.6638075,\n"," 'learning_rate': 0.07614928}\n","INFO:tensorflow:Step 8000 per-step time 0.169s\n","I1127 01:48:18.919701 135971017113600 model_lib_v2.py:705] Step 8000 per-step time 0.169s\n","INFO:tensorflow:{'Loss/classification_loss': 0.34452745,\n"," 'Loss/localization_loss': 0.21364136,\n"," 'Loss/regularization_loss': 0.17970099,\n"," 'Loss/total_loss': 0.7378698,\n"," 'learning_rate': 0.07603875}\n","I1127 01:48:18.920061 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.34452745,\n"," 'Loss/localization_loss': 0.21364136,\n"," 'Loss/regularization_loss': 0.17970099,\n"," 'Loss/total_loss': 0.7378698,\n"," 'learning_rate': 0.07603875}\n","INFO:tensorflow:Step 8100 per-step time 0.187s\n","I1127 01:48:37.581884 135971017113600 model_lib_v2.py:705] Step 8100 per-step time 0.187s\n","INFO:tensorflow:{'Loss/classification_loss': 0.25896606,\n"," 'Loss/localization_loss': 0.24719116,\n"," 'Loss/regularization_loss': 0.18001404,\n"," 'Loss/total_loss': 0.6861713,\n"," 'learning_rate': 0.07592674}\n","I1127 01:48:37.582179 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.25896606,\n"," 'Loss/localization_loss': 0.24719116,\n"," 'Loss/regularization_loss': 0.18001404,\n"," 'Loss/total_loss': 0.6861713,\n"," 'learning_rate': 0.07592674}\n","INFO:tensorflow:Step 8200 per-step time 0.177s\n","I1127 01:48:55.277851 135971017113600 model_lib_v2.py:705] Step 8200 per-step time 0.177s\n","INFO:tensorflow:{'Loss/classification_loss': 0.20615077,\n"," 'Loss/localization_loss': 0.18880194,\n"," 'Loss/regularization_loss': 0.18023935,\n"," 'Loss/total_loss': 0.5751921,\n"," 'learning_rate': 0.075813256}\n","I1127 01:48:55.278189 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.20615077,\n"," 'Loss/localization_loss': 0.18880194,\n"," 'Loss/regularization_loss': 0.18023935,\n"," 'Loss/total_loss': 0.5751921,\n"," 'learning_rate': 0.075813256}\n","INFO:tensorflow:Step 8300 per-step time 0.171s\n","I1127 01:49:12.367173 135971017113600 model_lib_v2.py:705] Step 8300 per-step time 0.171s\n","INFO:tensorflow:{'Loss/classification_loss': 0.37766042,\n"," 'Loss/localization_loss': 0.32928008,\n"," 'Loss/regularization_loss': 0.1805144,\n"," 'Loss/total_loss': 0.8874549,\n"," 'learning_rate': 0.07569829}\n","I1127 01:49:12.367514 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.37766042,\n"," 'Loss/localization_loss': 0.32928008,\n"," 'Loss/regularization_loss': 0.1805144,\n"," 'Loss/total_loss': 0.8874549,\n"," 'learning_rate': 0.07569829}\n","INFO:tensorflow:Step 8400 per-step time 0.169s\n","I1127 01:49:29.266584 135971017113600 model_lib_v2.py:705] Step 8400 per-step time 0.169s\n","INFO:tensorflow:{'Loss/classification_loss': 0.32290703,\n"," 'Loss/localization_loss': 0.2251649,\n"," 'Loss/regularization_loss': 0.18062188,\n"," 'Loss/total_loss': 0.7286938,\n"," 'learning_rate': 0.07558186}\n","I1127 01:49:29.266934 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.32290703,\n"," 'Loss/localization_loss': 0.2251649,\n"," 'Loss/regularization_loss': 0.18062188,\n"," 'Loss/total_loss': 0.7286938,\n"," 'learning_rate': 0.07558186}\n","INFO:tensorflow:Step 8500 per-step time 0.166s\n","I1127 01:49:45.913011 135971017113600 model_lib_v2.py:705] Step 8500 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.29195192,\n"," 'Loss/localization_loss': 0.18646841,\n"," 'Loss/regularization_loss': 0.1808008,\n"," 'Loss/total_loss': 0.6592211,\n"," 'learning_rate': 0.07546397}\n","I1127 01:49:45.913349 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.29195192,\n"," 'Loss/localization_loss': 0.18646841,\n"," 'Loss/regularization_loss': 0.1808008,\n"," 'Loss/total_loss': 0.6592211,\n"," 'learning_rate': 0.07546397}\n","INFO:tensorflow:Step 8600 per-step time 0.166s\n","I1127 01:50:02.511380 135971017113600 model_lib_v2.py:705] Step 8600 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.28942156,\n"," 'Loss/localization_loss': 0.21934612,\n"," 'Loss/regularization_loss': 0.18095382,\n"," 'Loss/total_loss': 0.68972147,\n"," 'learning_rate': 0.075344615}\n","I1127 01:50:02.511702 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.28942156,\n"," 'Loss/localization_loss': 0.21934612,\n"," 'Loss/regularization_loss': 0.18095382,\n"," 'Loss/total_loss': 0.68972147,\n"," 'learning_rate': 0.075344615}\n","INFO:tensorflow:Step 8700 per-step time 0.168s\n","I1127 01:50:19.312531 135971017113600 model_lib_v2.py:705] Step 8700 per-step time 0.168s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22187556,\n"," 'Loss/localization_loss': 0.14323005,\n"," 'Loss/regularization_loss': 0.18102638,\n"," 'Loss/total_loss': 0.546132,\n"," 'learning_rate': 0.07522382}\n","I1127 01:50:19.312827 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.22187556,\n"," 'Loss/localization_loss': 0.14323005,\n"," 'Loss/regularization_loss': 0.18102638,\n"," 'Loss/total_loss': 0.546132,\n"," 'learning_rate': 0.07522382}\n","INFO:tensorflow:Step 8800 per-step time 0.166s\n","I1127 01:50:35.935962 135971017113600 model_lib_v2.py:705] Step 8800 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2370567,\n"," 'Loss/localization_loss': 0.21539742,\n"," 'Loss/regularization_loss': 0.18105958,\n"," 'Loss/total_loss': 0.6335137,\n"," 'learning_rate': 0.07510157}\n","I1127 01:50:35.936259 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.2370567,\n"," 'Loss/localization_loss': 0.21539742,\n"," 'Loss/regularization_loss': 0.18105958,\n"," 'Loss/total_loss': 0.6335137,\n"," 'learning_rate': 0.07510157}\n","INFO:tensorflow:Step 8900 per-step time 0.164s\n","I1127 01:50:52.311331 135971017113600 model_lib_v2.py:705] Step 8900 per-step time 0.164s\n","INFO:tensorflow:{'Loss/classification_loss': 0.26353785,\n"," 'Loss/localization_loss': 0.27533147,\n"," 'Loss/regularization_loss': 0.18104881,\n"," 'Loss/total_loss': 0.71991813,\n"," 'learning_rate': 0.074977875}\n","I1127 01:50:52.311657 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.26353785,\n"," 'Loss/localization_loss': 0.27533147,\n"," 'Loss/regularization_loss': 0.18104881,\n"," 'Loss/total_loss': 0.71991813,\n"," 'learning_rate': 0.074977875}\n","INFO:tensorflow:Step 9000 per-step time 0.165s\n","I1127 01:51:08.812097 135971017113600 model_lib_v2.py:705] Step 9000 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.20356286,\n"," 'Loss/localization_loss': 0.15213266,\n"," 'Loss/regularization_loss': 0.18131799,\n"," 'Loss/total_loss': 0.53701353,\n"," 'learning_rate': 0.07485275}\n","I1127 01:51:08.812414 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.20356286,\n"," 'Loss/localization_loss': 0.15213266,\n"," 'Loss/regularization_loss': 0.18131799,\n"," 'Loss/total_loss': 0.53701353,\n"," 'learning_rate': 0.07485275}\n","INFO:tensorflow:Step 9100 per-step time 0.171s\n","I1127 01:51:25.879937 135971017113600 model_lib_v2.py:705] Step 9100 per-step time 0.171s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22565232,\n"," 'Loss/localization_loss': 0.19683434,\n"," 'Loss/regularization_loss': 0.18141045,\n"," 'Loss/total_loss': 0.6038971,\n"," 'learning_rate': 0.07472619}\n","I1127 01:51:25.880248 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.22565232,\n"," 'Loss/localization_loss': 0.19683434,\n"," 'Loss/regularization_loss': 0.18141045,\n"," 'Loss/total_loss': 0.6038971,\n"," 'learning_rate': 0.07472619}\n","INFO:tensorflow:Step 9200 per-step time 0.170s\n","I1127 01:51:42.927786 135971017113600 model_lib_v2.py:705] Step 9200 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2903569,\n"," 'Loss/localization_loss': 0.33328554,\n"," 'Loss/regularization_loss': 0.18161604,\n"," 'Loss/total_loss': 0.8052585,\n"," 'learning_rate': 0.07459819}\n","I1127 01:51:42.928136 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.2903569,\n"," 'Loss/localization_loss': 0.33328554,\n"," 'Loss/regularization_loss': 0.18161604,\n"," 'Loss/total_loss': 0.8052585,\n"," 'learning_rate': 0.07459819}\n","INFO:tensorflow:Step 9300 per-step time 0.165s\n","I1127 01:51:59.438019 135971017113600 model_lib_v2.py:705] Step 9300 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.27825153,\n"," 'Loss/localization_loss': 0.2537278,\n"," 'Loss/regularization_loss': 0.18171188,\n"," 'Loss/total_loss': 0.71369123,\n"," 'learning_rate': 0.074468784}\n","I1127 01:51:59.438401 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.27825153,\n"," 'Loss/localization_loss': 0.2537278,\n"," 'Loss/regularization_loss': 0.18171188,\n"," 'Loss/total_loss': 0.71369123,\n"," 'learning_rate': 0.074468784}\n","INFO:tensorflow:Step 9400 per-step time 0.164s\n","I1127 01:52:15.883386 135971017113600 model_lib_v2.py:705] Step 9400 per-step time 0.164s\n","INFO:tensorflow:{'Loss/classification_loss': 0.24963054,\n"," 'Loss/localization_loss': 0.24644476,\n"," 'Loss/regularization_loss': 0.18178509,\n"," 'Loss/total_loss': 0.6778604,\n"," 'learning_rate': 0.074337944}\n","I1127 01:52:15.883725 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.24963054,\n"," 'Loss/localization_loss': 0.24644476,\n"," 'Loss/regularization_loss': 0.18178509,\n"," 'Loss/total_loss': 0.6778604,\n"," 'learning_rate': 0.074337944}\n","INFO:tensorflow:Step 9500 per-step time 0.170s\n","I1127 01:52:32.875828 135971017113600 model_lib_v2.py:705] Step 9500 per-step time 0.170s\n","INFO:tensorflow:{'Loss/classification_loss': 0.30138412,\n"," 'Loss/localization_loss': 0.24383238,\n"," 'Loss/regularization_loss': 0.18172348,\n"," 'Loss/total_loss': 0.72694,\n"," 'learning_rate': 0.074205704}\n","I1127 01:52:32.876122 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.30138412,\n"," 'Loss/localization_loss': 0.24383238,\n"," 'Loss/regularization_loss': 0.18172348,\n"," 'Loss/total_loss': 0.72694,\n"," 'learning_rate': 0.074205704}\n","INFO:tensorflow:Step 9600 per-step time 0.165s\n","I1127 01:52:49.387259 135971017113600 model_lib_v2.py:705] Step 9600 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.396522,\n"," 'Loss/localization_loss': 0.20694941,\n"," 'Loss/regularization_loss': 0.18178536,\n"," 'Loss/total_loss': 0.78525674,\n"," 'learning_rate': 0.07407206}\n","I1127 01:52:49.387621 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.396522,\n"," 'Loss/localization_loss': 0.20694941,\n"," 'Loss/regularization_loss': 0.18178536,\n"," 'Loss/total_loss': 0.78525674,\n"," 'learning_rate': 0.07407206}\n","INFO:tensorflow:Step 9700 per-step time 0.165s\n","I1127 01:53:05.912631 135971017113600 model_lib_v2.py:705] Step 9700 per-step time 0.165s\n","INFO:tensorflow:{'Loss/classification_loss': 0.21170533,\n"," 'Loss/localization_loss': 0.14426391,\n"," 'Loss/regularization_loss': 0.18192479,\n"," 'Loss/total_loss': 0.537894,\n"," 'learning_rate': 0.073937014}\n","I1127 01:53:05.913010 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.21170533,\n"," 'Loss/localization_loss': 0.14426391,\n"," 'Loss/regularization_loss': 0.18192479,\n"," 'Loss/total_loss': 0.537894,\n"," 'learning_rate': 0.073937014}\n","INFO:tensorflow:Step 9800 per-step time 0.179s\n","I1127 01:53:23.863888 135971017113600 model_lib_v2.py:705] Step 9800 per-step time 0.179s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22219127,\n"," 'Loss/localization_loss': 0.19094503,\n"," 'Loss/regularization_loss': 0.18211183,\n"," 'Loss/total_loss': 0.5952481,\n"," 'learning_rate': 0.07380057}\n","I1127 01:53:23.864228 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.22219127,\n"," 'Loss/localization_loss': 0.19094503,\n"," 'Loss/regularization_loss': 0.18211183,\n"," 'Loss/total_loss': 0.5952481,\n"," 'learning_rate': 0.07380057}\n","INFO:tensorflow:Step 9900 per-step time 0.168s\n","I1127 01:53:40.706417 135971017113600 model_lib_v2.py:705] Step 9900 per-step time 0.168s\n","INFO:tensorflow:{'Loss/classification_loss': 0.26326582,\n"," 'Loss/localization_loss': 0.15521263,\n"," 'Loss/regularization_loss': 0.18231401,\n"," 'Loss/total_loss': 0.6007924,\n"," 'learning_rate': 0.073662736}\n","I1127 01:53:40.706742 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.26326582,\n"," 'Loss/localization_loss': 0.15521263,\n"," 'Loss/regularization_loss': 0.18231401,\n"," 'Loss/total_loss': 0.6007924,\n"," 'learning_rate': 0.073662736}\n","INFO:tensorflow:Step 10000 per-step time 0.166s\n","I1127 01:53:57.336266 135971017113600 model_lib_v2.py:705] Step 10000 per-step time 0.166s\n","INFO:tensorflow:{'Loss/classification_loss': 0.28836805,\n"," 'Loss/localization_loss': 0.25760677,\n"," 'Loss/regularization_loss': 0.18254434,\n"," 'Loss/total_loss': 0.7285192,\n"," 'learning_rate': 0.07352352}\n","I1127 01:53:57.336584 135971017113600 model_lib_v2.py:708] {'Loss/classification_loss': 0.28836805,\n"," 'Loss/localization_loss': 0.25760677,\n"," 'Loss/regularization_loss': 0.18254434,\n"," 'Loss/total_loss': 0.7285192,\n"," 'learning_rate': 0.07352352}\n"]}],"source":["# 모델 학습 실행\n","!python /content/models/research/object_detection/model_main_tf2.py \\\n","    --pipeline_config_path={pipeline_file} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --sample_1_of_n_eval_examples=1"]},{"cell_type":"markdown","source":["## 5-3. 훈련 끝난 모델 저장"],"metadata":{"id":"VYg81N9lfRPH"}},{"cell_type":"code","source":["import os\n","import re\n","import numpy as np\n","\n","\n","output_directory = \"/content/drive/MyDrive/moodels/fine_tuned_model\"  # 추론 모델 저장 경로\n","\n","# 최신 체크포인트 찾기\n","lst = os.listdir(model_dir)\n","lst = [l for l in lst if 'ckpt-' in l and '.index' in l]  # .index 파일만 필터링\n","steps = np.array([int(re.findall(r'\\d+', l)[0]) for l in lst])  # 체크포인트 번호 추출\n","latest_checkpoint = f\"ckpt-{steps.max()}\"  # 가장 큰 번호의 체크포인트 선택\n","latest_checkpoint_path = os.path.join(model_dir, latest_checkpoint)\n","\n","print(f\"Latest checkpoint: {latest_checkpoint_path}\")\n","\n","# 추론용 모델 저장\n","!python /content/models/research/object_detection/exporter_main_v2.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_file} \\\n","    --trained_checkpoint_dir={model_dir} \\\n","    --output_directory={output_directory}\n"],"metadata":{"id":"BfbZj0DZfV8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733108123681,"user_tz":-540,"elapsed":74542,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}},"outputId":"58b67425-614f-4828-f2b4-afd6bd3c5893"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Latest checkpoint: /content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite/ckpt-11\n","2024-12-02 02:54:10.336461: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2024-12-02 02:54:20.026857: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2024-12-02 02:54:20.026974: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","W1202 02:54:22.652605 138069178892288 deprecation.py:610] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","I1202 02:54:31.571306 138069178892288 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","I1202 02:54:45.654724 138069178892288 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","2024-12-02 02:54:48.989372: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","I1202 02:54:49.742612 138069178892288 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7d924c9b53c0>, because it is not built.\n","W1202 02:54:52.290115 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7d924c9b53c0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c9b5ff0>, because it is not built.\n","W1202 02:54:52.770799 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c9b5ff0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c44cc10>, because it is not built.\n","W1202 02:54:52.771070 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c44cc10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d924c44c790>, because it is not built.\n","W1202 02:54:52.771206 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d924c44c790>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c44e170>, because it is not built.\n","W1202 02:54:52.771313 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c44e170>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c44f3d0>, because it is not built.\n","W1202 02:54:52.771419 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c44f3d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d924c44d150>, because it is not built.\n","W1202 02:54:52.771526 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d924c44d150>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c921f30>, because it is not built.\n","W1202 02:54:52.771629 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c921f30>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c921c90>, because it is not built.\n","W1202 02:54:52.771734 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c921c90>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d924c922cb0>, because it is not built.\n","W1202 02:54:52.771831 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d924c922cb0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c455390>, because it is not built.\n","W1202 02:54:52.771928 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7d924c455390>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c454dc0>, because it is not built.\n","W1202 02:54:52.772042 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c454dc0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923ffbad10>, because it is not built.\n","W1202 02:54:52.772137 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923ffbad10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c9b65c0>, because it is not built.\n","W1202 02:54:52.772234 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c9b65c0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5e3e0>, because it is not built.\n","W1202 02:54:52.772334 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5e3e0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fd5c070>, because it is not built.\n","W1202 02:54:52.772430 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fd5c070>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5d600>, because it is not built.\n","W1202 02:54:52.772526 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5d600>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fd5d4e0>, because it is not built.\n","W1202 02:54:52.772621 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fd5d4e0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5c3a0>, because it is not built.\n","W1202 02:54:52.772722 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5c3a0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fd5dae0>, because it is not built.\n","W1202 02:54:52.772821 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fd5dae0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5dc60>, because it is not built.\n","W1202 02:54:52.772919 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5dc60>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c9b65f0>, because it is not built.\n","W1202 02:54:52.773045 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c9b65f0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5e680>, because it is not built.\n","W1202 02:54:52.773141 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fd5e680>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fde4df0>, because it is not built.\n","W1202 02:54:52.773236 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fde4df0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde4fd0>, because it is not built.\n","W1202 02:54:52.773330 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde4fd0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fde4a30>, because it is not built.\n","W1202 02:54:52.773425 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fde4a30>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde4910>, because it is not built.\n","W1202 02:54:52.773522 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde4910>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fde5480>, because it is not built.\n","W1202 02:54:52.773618 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fde5480>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde56f0>, because it is not built.\n","W1202 02:54:52.773725 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde56f0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c9b6620>, because it is not built.\n","W1202 02:54:52.773828 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d924c9b6620>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde74f0>, because it is not built.\n","W1202 02:54:52.773926 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fde74f0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda0280>, because it is not built.\n","W1202 02:54:52.774040 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda0280>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda0040>, because it is not built.\n","W1202 02:54:52.774135 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda0040>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda0c10>, because it is not built.\n","W1202 02:54:52.774227 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda0c10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda0af0>, because it is not built.\n","W1202 02:54:52.774406 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda0af0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda1180>, because it is not built.\n","W1202 02:54:52.774509 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda1180>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda1000>, because it is not built.\n","W1202 02:54:52.774612 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda1000>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda2d10>, because it is not built.\n","W1202 02:54:52.774711 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda2d10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3040>, because it is not built.\n","W1202 02:54:52.774818 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3040>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda3610>, because it is not built.\n","W1202 02:54:52.774917 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda3610>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3850>, because it is not built.\n","W1202 02:54:52.775035 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3850>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda37f0>, because it is not built.\n","W1202 02:54:52.775133 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda37f0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3640>, because it is not built.\n","W1202 02:54:52.775240 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3640>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda3d60>, because it is not built.\n","W1202 02:54:52.775336 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7d923fda3d60>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3d90>, because it is not built.\n","W1202 02:54:52.793519 138069178892288 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7d923fda3d90>, because it is not built.\n","W1202 02:55:13.355602 138069178892288 save.py:260] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 104). These functions will not be directly callable after loading.\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/moodels/fine_tuned_model/saved_model/assets\n","I1202 02:55:20.268092 138069178892288 builder_impl.py:779] Assets written to: /content/drive/MyDrive/moodels/fine_tuned_model/saved_model/assets\n","INFO:tensorflow:Writing pipeline config file to /content/drive/MyDrive/moodels/fine_tuned_model/pipeline.config\n","I1202 02:55:21.057559 138069178892288 config_util.py:253] Writing pipeline config file to /content/drive/MyDrive/moodels/fine_tuned_model/pipeline.config\n"]}]},{"cell_type":"code","source":["import os\n","\n","# 압축 대상 폴더 경로와 압축 파일 이름 설정\n","folder_to_zip = '/content/drive/MyDrive/moodels/fine_tuned_model/saved_model'  # 압축할 폴더 경로\n","output_zip = '/content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite/saved_model.zip'  # 생성될 ZIP 파일 경로\n","\n","# 폴더를 ZIP으로 압축\n","!zip -r {output_zip} {folder_to_zip}\n","\n","# 압축 완료 메시지 출력\n","print(f\"Folder '{folder_to_zip}' has been compressed to '{output_zip}'.\")"],"metadata":{"id":"Q1Ogm1Rtfcr9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733108184231,"user_tz":-540,"elapsed":1764,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}},"outputId":"a4a952bf-c8a9-4358-8210-51f9e2672fdf"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/drive/MyDrive/moodels/fine_tuned_model/saved_model/ (stored 0%)\n","  adding: content/drive/MyDrive/moodels/fine_tuned_model/saved_model/variables/ (stored 0%)\n","  adding: content/drive/MyDrive/moodels/fine_tuned_model/saved_model/variables/variables.data-00000-of-00001 (deflated 8%)\n","  adding: content/drive/MyDrive/moodels/fine_tuned_model/saved_model/variables/variables.index (deflated 78%)\n","  adding: content/drive/MyDrive/moodels/fine_tuned_model/saved_model/assets/ (stored 0%)\n","  adding: content/drive/MyDrive/moodels/fine_tuned_model/saved_model/saved_model.pb (deflated 92%)\n","Folder '/content/drive/MyDrive/moodels/fine_tuned_model/saved_model' has been compressed to '/content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite/saved_model.zip'.\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","\n","# 생성된 ZIP 파일 다운로드\n","files.download(output_zip)"],"metadata":{"id":"_UtouG_Bfj9B","colab":{"base_uri":"https://localhost:8080/","height":313},"executionInfo":{"status":"error","timestamp":1733108127941,"user_tz":-540,"elapsed":4,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}},"outputId":"bd792211-09dc-4a4c-d96c-4a539cc85090"},"execution_count":31,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"Cannot find file: /content/saved_model.zip","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-2d42e0a1fae2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 생성된 ZIP 파일 다운로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_zip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/saved_model.zip"]}]},{"cell_type":"markdown","metadata":{"id":"n4-uR1225Q6T"},"source":["# 6.&nbsp;TFLite 모델로 변환"]},{"cell_type":"markdown","metadata":{"id":"9upsDtWLB_Rt"},"source":["## 6-1. 훈련된 모델을 TFLite 모델로 내보내기"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57262,"status":"ok","timestamp":1733108273800,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"f9r9s7PS5TOT","outputId":"1b1048e5-ab31-4801-aa14-1632cd47ae35"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/custom_model_lite’: File exists\n","2024-12-02 02:56:57.130563: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2024-12-02 02:57:02.415243: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2024-12-02 02:57:02.415303: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","I1202 02:57:07.676317 137396488953856 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","I1202 02:57:15.961182 137396488953856 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","2024-12-02 02:57:19.139019: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","I1202 02:57:19.954946 137396488953856 api.py:441] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7cf5afd7ff10>, because it is not built.\n","W1202 02:57:21.435311 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7cf5afd7ff10>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5ad2f4cd0>, because it is not built.\n","W1202 02:57:21.697246 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5ad2f4cd0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acd03040>, because it is not built.\n","W1202 02:57:21.697497 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acd03040>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad187970>, because it is not built.\n","W1202 02:57:21.697631 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad187970>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5ad2417e0>, because it is not built.\n","W1202 02:57:21.697761 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5ad2417e0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad010790>, because it is not built.\n","W1202 02:57:21.697868 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad010790>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad010040>, because it is not built.\n","W1202 02:57:21.697991 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad010040>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5ad013c70>, because it is not built.\n","W1202 02:57:21.698098 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5ad013c70>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad012590>, because it is not built.\n","W1202 02:57:21.698199 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad012590>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad013a00>, because it is not built.\n","W1202 02:57:21.698296 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad013a00>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5acfd2bc0>, because it is not built.\n","W1202 02:57:21.698392 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.convolutional.SeparableConv2D object at 0x7cf5acfd2bc0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acfd02e0>, because it is not built.\n","W1202 02:57:21.698488 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acfd02e0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad106b00>, because it is not built.\n","W1202 02:57:21.698584 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ad106b00>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad2f52a0>, because it is not built.\n","W1202 02:57:21.698678 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad2f52a0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acd67b80>, because it is not built.\n","W1202 02:57:21.698785 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acd67b80>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5accf1120>, because it is not built.\n","W1202 02:57:21.698883 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5accf1120>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5accf24a0>, because it is not built.\n","W1202 02:57:21.698999 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5accf24a0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5accf38b0>, because it is not built.\n","W1202 02:57:21.699103 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5accf38b0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5accf1330>, because it is not built.\n","W1202 02:57:21.699203 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5accf1330>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5accf3ac0>, because it is not built.\n","W1202 02:57:21.699301 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5accf3ac0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5accf3070>, because it is not built.\n","W1202 02:57:21.699398 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5accf3070>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad2f52d0>, because it is not built.\n","W1202 02:57:21.699495 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad2f52d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac9b02e0>, because it is not built.\n","W1202 02:57:21.699591 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac9b02e0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acb2d840>, because it is not built.\n","W1202 02:57:21.699689 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acb2d840>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2d630>, because it is not built.\n","W1202 02:57:21.699797 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2d630>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acb2d810>, because it is not built.\n","W1202 02:57:21.699895 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acb2d810>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2df60>, because it is not built.\n","W1202 02:57:21.700010 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2df60>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acb2f4c0>, because it is not built.\n","W1202 02:57:21.700109 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5acb2f4c0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2c640>, because it is not built.\n","W1202 02:57:21.700206 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2c640>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad2f5300>, because it is not built.\n","W1202 02:57:21.700312 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ad2f5300>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2ead0>, because it is not built.\n","W1202 02:57:21.700410 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5acb2ead0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac890ac0>, because it is not built.\n","W1202 02:57:21.700507 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac890ac0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac890eb0>, because it is not built.\n","W1202 02:57:21.700604 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac890eb0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac890ee0>, because it is not built.\n","W1202 02:57:21.700699 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac890ee0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac890130>, because it is not built.\n","W1202 02:57:21.701037 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac890130>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac893070>, because it is not built.\n","W1202 02:57:21.701180 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac893070>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac890d30>, because it is not built.\n","W1202 02:57:21.701291 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac890d30>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac8920e0>, because it is not built.\n","W1202 02:57:21.701395 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac8920e0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac891300>, because it is not built.\n","W1202 02:57:21.701508 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac891300>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac7ec1c0>, because it is not built.\n","W1202 02:57:21.701609 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac7ec1c0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac7ec0d0>, because it is not built.\n","W1202 02:57:21.701719 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac7ec0d0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac7ecc70>, because it is not built.\n","W1202 02:57:21.701820 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac7ecc70>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac7ecb50>, because it is not built.\n","W1202 02:57:21.702037 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac7ecb50>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac7eceb0>, because it is not built.\n","W1202 02:57:21.702142 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7cf5ac7eceb0>, because it is not built.\n","WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac7ed030>, because it is not built.\n","W1202 02:57:21.782047 137396488953856 save_impl.py:71] Skipping full serialization of Keras layer <keras.layers.core.lambda_layer.Lambda object at 0x7cf5ac7ed030>, because it is not built.\n","W1202 02:57:42.736368 137396488953856 save.py:260] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 104). These functions will not be directly callable after loading.\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/models/saved_model/assets\n","I1202 02:57:50.173300 137396488953856 builder_impl.py:779] Assets written to: /content/drive/MyDrive/models/saved_model/assets\n"]}],"source":["# 훈련된 TFLite 모델을 저장할 디렉토리 생성\n","!mkdir /content/custom_model_lite\n","output_directory = '/content/drive/MyDrive/models'\n","\n","# 훈련 디렉토리 경로 설정 (변환 스크립트가 가장 최신 체크포인트 파일을 자동으로 선택)\n","last_model_path = '/content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite'\n","\n","# TFLite 변환 그래프를 생성\n","!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n","    --trained_checkpoint_dir {last_model_path} \\\n","    --output_directory {output_directory} \\\n","    --pipeline_config_path {pipeline_file}\n"]},{"cell_type":"markdown","metadata":{"id":"kbBlzaUaCI-z"},"source":["## 6-2. TensorFlow Lite 모델 저장 및 테스트"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"vEA397hI5XGV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1733108334781,"user_tz":-540,"elapsed":20176,"user":{"displayName":"CrossMate","userId":"02980242039632566855"}},"outputId":"8b4847cd-9794-4a18-a8d6-067b0d02140d"},"outputs":[{"output_type":"error","ename":"ConverterError","evalue":"<unknown>:0: error: loc(callsite(callsite(fused[\"ConcatV2:\", \"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame/Scale/concat@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.ConcatV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"ConcatV2:\", \"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame/Scale/concat@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(fused[\"StridedSlice:\", \"map/while/strided_slice@map_while_body_7869\"] at callsite(callsite(fused[\"StatelessWhile:\", \"map/while@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]))): 'tf.StridedSlice' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(callsite(callsite(fused[\"StatelessWhile:\", \"map/while@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): called from\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(fused[\"StridedSlice:\", \"map/while/strided_slice@map_while_body_7869\"] at callsite(callsite(fused[\"StatelessWhile:\", \"map/while@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]))): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: ConcatV2, StridedSlice\nDetails:\n\ttf.ConcatV2(tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<i32>) -> (tensor<4xf32>) : {device = \"\"}\n\ttf.StridedSlice(tensor<?x?x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<1x?x?x3xf32>) : {begin_mask = 14 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 14 : i64, new_axis_mask = 1 : i64, shrink_axis_mask = 0 : i64}\n\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-d1aa1dd3f281>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# TFLite 형식으로 모델 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TFLite 모델로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 변환된 TFLite 모델을 파일로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1082\u001b[0m           graph_def)\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_from_saved_model\u001b[0;34m(self, graph_def)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0mconverter_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconverter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m     return self._optimize_tflite_model(\n\u001b[1;32m    969\u001b[0m         result, quant_mode, quant_io=self.experimental_new_quantizer)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert_saved_model\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m   \u001b[0mmodel_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m   \u001b[0mconversion_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_conversion_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m   data = convert(\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0mmodel_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0mconversion_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    304\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0merror_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_metrics_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_collected_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m   return _run_deprecated_conversion_binary(\n","\u001b[0;31mConverterError\u001b[0m: <unknown>:0: error: loc(callsite(callsite(fused[\"ConcatV2:\", \"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame/Scale/concat@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.ConcatV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"ConcatV2:\", \"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame/Scale/concat@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(fused[\"StridedSlice:\", \"map/while/strided_slice@map_while_body_7869\"] at callsite(callsite(fused[\"StatelessWhile:\", \"map/while@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]))): 'tf.StridedSlice' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(callsite(callsite(fused[\"StatelessWhile:\", \"map/while@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): called from\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(fused[\"StridedSlice:\", \"map/while/strided_slice@map_while_body_7869\"] at callsite(callsite(fused[\"StatelessWhile:\", \"map/while@__inference_call_func_11661\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_14035\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]))): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: ConcatV2, StridedSlice\nDetails:\n\ttf.ConcatV2(tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<i32>) -> (tensor<4xf32>) : {device = \"\"}\n\ttf.StridedSlice(tensor<?x?x3xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<1x?x?x3xf32>) : {begin_mask = 14 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 14 : i64, new_axis_mask = 1 : i64, shrink_axis_mask = 0 : i64}\n\n"]}],"source":["# 내보낸 그래프 파일을 TFLite 모델 파일로 변환\n","\n","import tensorflow as tf\n","\n","# 저장된 모델(saved_model) 경로를 지정하여 TFLiteConverter 생성\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/MyDrive/moodels/fine_tuned_model/saved_model')  # 저장된 모델 로드\n","\n","# TFLite 형식으로 모델 변환\n","tflite_model = converter.convert()  # TFLite 모델로 변환\n","\n","# 변환된 TFLite 모델을 파일로 저장\n","with open('/content/drive/MyDrive/models/ssd_mobilenetv2_fpnlite/detect.tflite', 'wb') as f:  # TFLite 파일 경로 지정\n","  f.write(tflite_model)  # 변환된 TFLite 모델 데이터를 파일에 작성"]},{"cell_type":"markdown","metadata":{"id":"-rv3dwlh5Y1W"},"source":["# 7.&nbsp;TFLite 모델 테스트 및 평가"]},{"cell_type":"markdown","metadata":{"id":"B4Doj5xOCkwc"},"source":["## 7-1. TFLite 모델을 이용한 객체 탐지 수행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9i1Xdls5YpL"},"outputs":[],"source":["# 사용자 정의 TFLite 모델을 테스트 이미지에서 실행하여 객체를 탐지하는 스크립트\n","# 출처: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n","\n","# 필요한 패키지 임포트\n","import os\n","import cv2  # 이미지 처리 라이브러리\n","import numpy as np\n","import sys\n","import glob  # 파일 경로를 다룰 때 사용\n","import random  # 테스트 이미지를 무작위로 선택\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter  # TFLite 모델 해석기\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# Jupyter/Colab에서 이미지를 인라인으로 표시\n","%matplotlib inline\n","\n","# TFLite 모델을 사용하여 이미지에서 객체를 탐지하고 결과를 표시하는 함수 정의\n","def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n","    # 테스트 폴더에서 이미지 파일 경로를 모두 가져옴\n","    images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n","\n","    # 라벨 맵 파일 로드\n","    with open(lblpath, 'r') as f:\n","        labels = [line.strip() for line in f.readlines()]  # 각 라인을 클래스 이름으로 저장\n","\n","    # TensorFlow Lite 모델 로드\n","    interpreter = Interpreter(model_path=modelpath)\n","    interpreter.allocate_tensors()  # 모델에서 필요한 텐서를 메모리에 할당\n","\n","    # 모델 입력 및 출력 세부정보 가져오기\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    height = input_details[0]['shape'][1]  # 입력 이미지 높이\n","    width = input_details[0]['shape'][2]   # 입력 이미지 너비\n","\n","    float_input = (input_details[0]['dtype'] == np.float32)  # 입력 데이터 유형이 float인지 확인\n","\n","    input_mean = 127.5  # 이미지 정규화를 위한 평균 값\n","    input_std = 127.5   # 이미지 정규화를 위한 표준 편차 값\n","\n","    # 테스트 이미지 무작위 선택\n","    images_to_test = random.sample(images, num_test_images)\n","\n","    # 각 이미지에 대해 탐지 수행\n","    for image_path in images_to_test:\n","        # 이미지 로드 및 모델 입력 크기로 리사이즈\n","        image = cv2.imread(image_path)  # 이미지를 읽어옴\n","        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # RGB로 변환\n","        imH, imW, _ = image.shape  # 원본 이미지 크기 저장\n","        image_resized = cv2.resize(image_rgb, (width, height))  # 모델 입력 크기로 리사이즈\n","        input_data = np.expand_dims(image_resized, axis=0)  # 배치를 추가하여 [1xHxWx3] 형태로 만듦\n","\n","        # 모델이 float 입력을 기대하면 정규화 수행\n","        if float_input:\n","            input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","        # 모델에 데이터를 입력하고 탐지 수행\n","        interpreter.set_tensor(input_details[0]['index'], input_data)\n","        interpreter.invoke()  # 모델 실행\n","\n","        # 탐지 결과 가져오기\n","        boxes = interpreter.get_tensor(output_details[1]['index'])[0]  # 탐지된 객체의 바운딩 박스 좌표\n","        classes = interpreter.get_tensor(output_details[3]['index'])[0]  # 탐지된 객체의 클래스 인덱스\n","        scores = interpreter.get_tensor(output_details[0]['index'])[0]  # 탐지된 객체의 신뢰도\n","\n","        detections = []  # 탐지 결과 저장\n","\n","        # 탐지 결과를 바탕으로 바운딩 박스를 그리거나 결과를 저장\n","        for i in range(len(scores)):\n","            if ((scores[i] > min_conf) and (scores[i] <= 1.0)):  # 최소 신뢰도 임계값 이상인 경우만 처리\n","                # 바운딩 박스 좌표 계산 (이미지 크기를 고려하여 조정)\n","                ymin = int(max(1, (boxes[i][0] * imH)))\n","                xmin = int(max(1, (boxes[i][1] * imW)))\n","                ymax = int(min(imH, (boxes[i][2] * imH)))\n","                xmax = int(min(imW, (boxes[i][3] * imW)))\n","\n","                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)  # 바운딩 박스 그리기\n","\n","                # 객체 이름과 신뢰도 표시\n","                object_name = labels[int(classes[i])]  # 클래스 이름 가져오기\n","                label = '%s: %d%%' % (object_name, int(scores[i] * 100))  # 예: 'person: 72%'\n","                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n","                label_ymin = max(ymin, labelSize[1] + 10)\n","                cv2.rectangle(image, (xmin, label_ymin - labelSize[1] - 10), (xmin + labelSize[0], label_ymin + baseLine - 10), (255, 255, 255), cv2.FILLED)\n","                cv2.putText(image, label, (xmin, label_ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n","\n","                detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n","\n","        # 탐지된 결과를 이미지로 표시하거나 저장\n","        if txt_only == False:  # 이미지 결과를 표시\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            plt.figure(figsize=(12, 16))\n","            plt.imshow(image)\n","            plt.show()\n","\n","        elif txt_only == True:  # 텍스트 파일로 저장\n","            image_fn = os.path.basename(image_path)\n","            base_fn, ext = os.path.splitext(image_fn)\n","            txt_result_fn = base_fn + '.txt'\n","            txt_savepath = os.path.join(savepath, txt_result_fn)\n","\n","            # 탐지 결과를 텍스트 파일에 작성\n","            with open(txt_savepath, 'w') as f:\n","                for detection in detections:\n","                    f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n","\n","    return"]},{"cell_type":"markdown","metadata":{"id":"9-pehif0O5ai"},"source":["- `PATH_TO_IMAGES` 경로 지정\n","- `images_to_test` 탐지 실행할 이미지 개수 지정"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HLtqRUPO5iYe"},"outputs":[],"source":["# 사용자 모델 실행을 위한 변수 설정\n","\n","# 테스트 이미지 폴더 경로\n","PATH_TO_IMAGES = '/content/drive/MyDrive/nipa_google/data/images_52'  # 테스트 이미지가 저장된 폴더 경로\n","\n","# TFLite 모델 파일 경로\n","PATH_TO_MODEL = '/content/custom_model_lite/detect.tflite'  # 변환된 TFLite 모델 파일 경로\n","\n","# 라벨 맵 파일 경로\n","PATH_TO_LABELS = '/content/labelmap.txt'  # 클래스 이름이 정의된 labelmap.txt 파일 경로\n","\n","# 최소 신뢰도 임계값 설정\n","min_conf_threshold = 0.3  # 탐지된 객체의 신뢰도가 30% 이상인 경우만 표시\n","# 신뢰도가 낮은 탐지 결과도 확인하려면 값을 낮춰 예: 0.01로 변경 가능\n","\n","# 테스트에 사용할 이미지 수 설정\n","images_to_test = 52  # 탐지를 실행할 이미지 개수\n","\n","# 객체 탐지 함수 실행\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)\n","# TFLite 모델, 이미지 폴더, 라벨 파일, 신뢰도 임계값, 테스트 이미지 개수를 입력으로 전달"]},{"cell_type":"markdown","metadata":{"id":"BMOUGBZbCos0"},"source":["## 7-2. mAP(mean Average Precision) 계산을 통한 모델 성능 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2276,"status":"ok","timestamp":1732672791915,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"},"user_tz":-540},"id":"X8BnoBlE5l6B","outputId":"6ec88202-77c3-4bab-feb2-77bede80f653"},"outputs":[{"output_type":"stream","name":"stderr","text":["Cloning into '/content/mAP'...\n","--2024-11-27 01:59:50--  https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5397 (5.3K) [text/plain]\n","Saving to: ‘calculate_map_cartucho.py’\n","\n","     0K .....                                                 100% 51.6M=0s\n","\n","2024-11-27 01:59:50 (51.6 MB/s) - ‘calculate_map_cartucho.py’ saved [5397/5397]\n","\n"]}],"source":["%%bash\n","# mAP(mAP - mean Average Precision) 계산을 위한 GitHub 저장소 클론\n","git clone https://github.com/Cartucho/mAP /content/mAP\n","# Cartucho의 mAP 프로젝트를 로컬로 복사하여 /content/mAP 디렉토리에 저장\n","\n","# mAP 디렉토리로 이동\n","cd /content/mAP\n","\n","# 기존 탐지 결과(detection-results) 폴더 초기화\n","rm input/detection-results/*\n","\n","# 기존 Ground Truth(정답 데이터) 폴더 초기화\n","rm input/ground-truth/*\n","\n","# 선택적 이미지 폴더 초기화\n","rm input/images-optional/*\n","\n","# mAP 계산에 필요한 유틸리티 스크립트 다운로드\n","wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py\n","# EdjeElectronics의 GitHub에서 mAP 계산을 도와주는 Python 스크립트를 다운로드"]},{"cell_type":"markdown","metadata":{"id":"jJfRgkM7UpwC"},"source":["- 아래 코드의 '이미지와 XML 파일 복사' 코드에 `PATH_TO_IMAGES` 경로 지정\n","  - !cp `PATH_TO_IMAGES`* /content/mAP/input/images-optional"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXxPhQkF5pT5"},"outputs":[],"source":["# 테스트 이미지 및 관련 XML 파일을 mAP 평가 환경으로 복사 및 이동\n","\n","# 이미지와 XML 파일 복사\n","!cp /content/drive/MyDrive/nipa_google/data/images_52/* /content/mAP/input/images-optional\n","# 테스트 이미지와 관련 XML 파일을 mAP의 선택적 이미지 폴더(`images-optional`)로 복사\n","\n","# XML 파일 이동\n","!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/\n","# 복사된 XML 파일을 Ground Truth 데이터 폴더(`ground-truth`)로 이동\n","# Ground Truth 폴더는 실제 라벨 데이터를 저장하는 폴더로, mAP 계산에 사용됨"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1732672799236,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"},"user_tz":-540},"id":"TEtgYgfs5q7Q","outputId":"36ae3b17-69b0-4abb-9ef8-75016c804433"},"outputs":[{"output_type":"stream","name":"stdout","text":["Conversion completed!\n"]}],"source":["# Ground Truth XML 파일을 텍스트 파일로 변환\n","\n","!python /content/mAP/scripts/extra/convert_gt_xml.py\n","# Pascal VOC 형식의 Ground Truth XML 파일을\n","# mAP 계산 도구에서 사용하는 텍스트 파일 형식으로 변환하는 스크립트를 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3299,"status":"ok","timestamp":1732672804976,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"},"user_tz":-540},"id":"m9H2Z8LG5s7f","outputId":"f561ef71-ca13-42f5-9f98-ee146ef11ff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting inference on 52 images...\n","Finished inferencing!\n"]}],"source":["# 탐지를 실행하여 결과를 텍스트 파일로 저장하기 위한 변수 설정\n","\n","# 테스트 이미지 폴더 경로\n","PATH_TO_IMAGES = '/content/drive/MyDrive/nipa_google/data/images_52'  # 테스트 이미지가 저장된 폴더 경로\n","\n","# TFLite 모델 파일 경로\n","PATH_TO_MODEL = '/content/custom_model_lite/detect.tflite'  # 변환된 TFLite 모델 파일 경로\n","\n","# 라벨 맵 파일 경로\n","PATH_TO_LABELS = '/content/labelmap.txt'  # 클래스 이름이 정의된 labelmap.txt 파일 경로\n","\n","# 탐지 결과를 저장할 폴더 경로\n","PATH_TO_RESULTS = '/content/mAP/input/detection-results'  # 탐지 결과 텍스트 파일을 저장할 디렉토리\n","\n","# 최소 신뢰도 임계값 설정\n","min_conf_threshold = 0.1  # 탐지된 객체의 신뢰도가 10% 이상인 경우만 저장\n","\n","# 테스트 폴더에 있는 모든 이미지 파일 경로를 가져오기\n","image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n","images_to_test = min(500, len(image_list))  # 이미지가 500개 이상이면 상한선을 500개로 설정\n","\n","# 탐지 결과만 텍스트 파일로 저장하도록 설정 (이미지 표시하지 않음)\n","txt_only = True\n","\n","# 탐지 함수 실행\n","print('Starting inference on %d images...' % images_to_test)  # 탐지 시작 메시지 출력\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n","# TFLite 모델, 이미지 폴더, 라벨 파일, 신뢰도 임계값, 테스트 이미지 수, 탐지 결과 저장 경로를 입력으로 전달\n","print('Finished inferencing!')  # 탐지 완료 메시지 출력"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3192,"status":"ok","timestamp":1732672810404,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"},"user_tz":-540},"id":"95DYb3095vI9","outputId":"c1f380c1-8f50-46d8-af79-c552ed4b2fb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/mAP\n","Calculating mAP at 0.50 IoU threshold...\n","13.95% = bicycle AP \n","20.29% = kickboard AP \n","24.45% = motorcycle AP \n","mAP = 19.56%\n","Calculating mAP at 0.55 IoU threshold...\n","7.10% = bicycle AP \n","20.29% = kickboard AP \n","22.62% = motorcycle AP \n","mAP = 16.67%\n","Calculating mAP at 0.60 IoU threshold...\n","6.19% = bicycle AP \n","20.29% = kickboard AP \n","20.25% = motorcycle AP \n","mAP = 15.58%\n","Calculating mAP at 0.65 IoU threshold...\n","5.75% = bicycle AP \n","20.29% = kickboard AP \n","16.72% = motorcycle AP \n","mAP = 14.26%\n","Calculating mAP at 0.70 IoU threshold...\n","5.05% = bicycle AP \n","14.93% = kickboard AP \n","14.19% = motorcycle AP \n","mAP = 11.39%\n","Calculating mAP at 0.75 IoU threshold...\n","4.47% = bicycle AP \n","14.11% = kickboard AP \n","8.51% = motorcycle AP \n","mAP = 9.03%\n","Calculating mAP at 0.80 IoU threshold...\n","1.48% = bicycle AP \n","5.58% = kickboard AP \n","3.02% = motorcycle AP \n","mAP = 3.36%\n","Calculating mAP at 0.85 IoU threshold...\n","0.25% = bicycle AP \n","0.95% = kickboard AP \n","1.07% = motorcycle AP \n","mAP = 0.76%\n","Calculating mAP at 0.90 IoU threshold...\n","0.00% = bicycle AP \n","0.95% = kickboard AP \n","0.00% = motorcycle AP \n","mAP = 0.32%\n","Calculating mAP at 0.95 IoU threshold...\n","0.00% = bicycle AP \n","0.00% = kickboard AP \n","0.00% = motorcycle AP \n","mAP = 0.00%\n","\n","***mAP Results***\n","\n","Class\t\tAverage mAP @ 0.5:0.95\n","---------------------------------------\n","bicycle\t\t4.42%\n","kickboard\t\t11.77%\n","motorcycle\t\t11.08%\n","\n","Overall\t\t9.09%\n"]}],"source":["# mAP 계산을 위한 디렉토리 이동\n","%cd /content/mAP\n","\n","# mAP(mean Average Precision) 계산 스크립트 실행\n","!python calculate_map_cartucho.py --labels=/content/labelmap.txt\n","# calculate_map_cartucho.py 스크립트를 실행하여 mAP 계산\n","# --labels 옵션은 라벨 맵 파일(labelmap.txt)의 경로를 지정"]},{"cell_type":"markdown","metadata":{"id":"whhoNjj7525I"},"source":["# 8.&nbsp;TFLite 모델 배포"]},{"cell_type":"markdown","metadata":{"id":"3YWcHALc550E"},"source":["## 8-1. TFLite 모델 및 관련 파일 압축 및 다운로드"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":547,"status":"ok","timestamp":1733107534008,"user":{"displayName":"CrossMate","userId":"02980242039632566855"},"user_tz":-540},"id":"QBoxpB1E509-","outputId":"669e8c99-7fd5-4649-da74-6fa5b167f408"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/labelmap.pbtxt': No such file or directory\n","cp: cannot stat '/content/models/mymodel/pipeline_file.config': No such file or directory\n","/content\n","  adding: custom_model_lite (deflated 10%)\n"]}],"source":["# 라벨 맵 파일과 파이프라인 구성 파일을 TFLite 모델 폴더로 이동한 후 압축하기\n","\n","# 라벨 맵 파일(labelmap.txt)을 TFLite 모델 폴더로 복사\n","!cp /content/labelmap.txt /content/custom_model_lite\n","\n","# 라벨 맵 파일(labelmap.pbtxt)을 TFLite 모델 폴더로 복사\n","!cp /content/labelmap.pbtxt /content/custom_model_lite\n","\n","# 파이프라인 구성 파일(pipeline_file.config)을 TFLite 모델 폴더로 복사\n","!cp /content/models/mymodel/pipeline_file.config /content/custom_model_lite\n","\n","# 현재 디렉토리를 /content로 변경\n","%cd /content\n","\n","# TFLite 모델 폴더(custom_model_lite)를 압축하여 ZIP 파일 생성\n","!zip -r custom_model_lite.zip custom_model_lite\n","# -r 옵션: 폴더 및 하위 파일을 모두 포함하여 압축\n","# 결과 ZIP 파일(custom_model_lite.zip)은 /content 디렉토리에 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1732673066031,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"},"user_tz":-540},"id":"ly9-GC-w6AFg","outputId":"305c3eb9-a313-4b6f-81f8-8957accd73b6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_11f463a5-fc3e-421d-b8ca-03270d35a19c\", \"custom_model_lite.zip\", 20783554)"]},"metadata":{}}],"source":["from google.colab import files\n","\n","# 압축된 TFLite 모델 ZIP 파일을 다운로드\n","files.download('/content/custom_model_lite.zip')"]},{"cell_type":"markdown","source":["## 8-2. 모델 정보 확인"],"metadata":{"id":"DhnVKWsqvB3N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3wftZCuXQg8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732679507326,"user_tz":-540,"elapsed":273,"user":{"displayName":"코딩 부계정","userId":"17521845357504763889"}},"outputId":"4d112304-1725-40fd-9eb2-e3f4a8674f28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input details: [{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1, 320, 320,   3], dtype=int32), 'shape_signature': array([  1, 320, 320,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n","Output details: [{'name': 'StatefulPartitionedCall:1', 'index': 338, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:3', 'index': 336, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 339, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 337, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"]}],"source":["import tensorflow as tf\n","\n","# TFLite 모델 로드\n","interpreter = tf.lite.Interpreter(model_path=\"/content/detect.tflite\")\n","interpreter.allocate_tensors()\n","\n","# 입력 정보\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","print(\"Input details:\", input_details)\n","print(\"Output details:\", output_details)"]},{"cell_type":"code","source":[],"metadata":{"id":"OOUT4SVevikM"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["hSJCDeRPn89q","LiK39Hce2EVV","-rv3dwlh5Y1W"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}